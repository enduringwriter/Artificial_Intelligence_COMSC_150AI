{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0008556c-2809-4686-b7b7-16c7d68bb8db",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Neural networks using MLPClassifier of sklearn.neural_network module\n",
    "\n",
    "### Classifying (recognizing) hand-written digits using neural networks\n",
    "\n",
    "### Objectives\n",
    "\n",
    "On completing this assignment, students will be able to write a simple AI application for classifying (recognizing) hand-written digits using neural networks.\n",
    "\n",
    "\n",
    "### Readings for Neural Networks\n",
    "\n",
    "#### Required Reading: Introduction\n",
    "\n",
    "https://www.ibm.com/topics/neural-networks#:~:text=One%20of%20the%20best%2Dknown,heart%20of%20deep%20learning%20models.\n",
    "\n",
    "#### Optional Reading: Backpropagation\n",
    "\n",
    "https://www.geeksforgeeks.org/backpropagation-in-neural-network\n",
    "\n",
    "#### Required Reading: Code Example\n",
    "\n",
    "https://www.pluralsight.com/resources/blog/guides/machine-learning-neural-networks-scikit-learn#:~:text=In%20this%20guide%2C%20you%20have,training%20and%20test%20data%2C%20respectivetiv\n",
    "\n",
    "\n",
    "### Assignment\n",
    " \n",
    "Write an AI application which will classify hand-written digits into numerals 0 to 9 using neural networks. This application is similar to the one you did previously. However, in this application, each image is represented with 28 by 28 pixels (instead of with 8 by 8 pixels). The data used in this application and steps to follow to write the application are outlined below.\n",
    "\n",
    "### Loading data set and viewing its values\n",
    "\n",
    "The data used in this assignment is provided in the following two files.\n",
    "\n",
    "    images.pkl\n",
    "    targets.pkl\n",
    "\n",
    "The first file contains an array of shape = (60000, 28, 28). The second file contains an array of shape (60000,). We extract these arrays from the above files and store their contents into variables images and target. Then display different characteristics of those variables as shown in the code below. \n",
    "\n",
    "    \n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "    f = open (\"images.pkl\", 'rb')\n",
    "    images = pickle.load(f)\n",
    "    f.close()\n",
    "    f = open (\"target.pkl\", 'rb')\n",
    "    target = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    print (type (images))\n",
    "    print (type (target))\n",
    "    print(images.shape)\n",
    "    print(target.shape)\n",
    "    print (images[0])\n",
    "    print (target)\n",
    "    print (target[0])\n",
    "\n",
    "\n",
    "    The output of the above code is below:\n",
    "\n",
    "    <class 'numpy.ndarray'>\n",
    "    <class 'numpy.ndarray'>\n",
    "    (60000, 28, 28)\n",
    "    (60000,)\n",
    "    [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255 247 127   0   0   0   0] \n",
    "    [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251 93  82  82  56  39   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241 0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154 0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119 25   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253 253 207   2   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201 78   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]\n",
    "     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
    "    0   0   0   0   0   0   0   0   0   0]]\n",
    "    \n",
    "    [5 0 4 ... 5 6 8]\n",
    "    5\n",
    "\n",
    "\n",
    "### Results of viewing the above output\n",
    "\n",
    "By studying the above output, we conclude the following:\n",
    "\n",
    "- The arrays, images and target, are numpy arrays\n",
    "- The shape of array images is (60000, 28, 28). So, the array contains 60000 images and each image is 28 by 28 two dimensional array.\n",
    "- The array target is a one dimensional array and it contains integers, varying in values between 0 and 9 and these values correspond to images stored in array images. Above, when we display the first element of target array, it is 5. Below, we display the first image in array images and it is an image of 5. See below.\n",
    "\n",
    "#### Displaying an image\n",
    "\n",
    "Below, we display the first image in array images.\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.matshow(images[0])\n",
    "\n",
    "The image shown by above code would be the image of 5.\n",
    "\n",
    "#### Conclusions from above\n",
    "\n",
    "From above, we conclude the following: \n",
    "\n",
    "- We should leave the array target as it is and use it for target values\n",
    "- We should also leave the array images as it is and use it for displaying images with matplotlib.pyplot or plt as shown above.\n",
    "- Using array images of shape (60000, 28, 28), we should create another array named data_flat of shape (60000, 784) (where 784 = 28 * 28) by simply reshaping the array images. This would make each image to be a one dimensional array of size 784 containing 784 pixel values and we would be able to use those image values as input to neural networks. (Neural networks require that their inputs be flat or one dimensional). Below, we create the array data_flat.\n",
    "\n",
    "\n",
    "#### Create flat array\n",
    "\n",
    "Below, we reshape the array images of shape = (60000, 28, 28) into an array of shape = (6000 , 784) and save it as array data_flat.                     \n",
    "\n",
    "    data_flat = images.reshape(len(images),28 *28)\n",
    "    print (data_flat.shape)\n",
    "\n",
    "    output from above:\n",
    "    (60000, 784)\n",
    "    \n",
    "\n",
    "#### Normalizing the flat array\n",
    "\n",
    "The pixel values in array data_flat vary from 0 to 255. We should normalize this array by dividing each of its values by 255. So, the values of converted array data_flat_norm will vary from 0 to 1. Since, data_flat is a numpy array, we can accomplish this in single step as shown in the code below. We also display the values of one element of the converted array, data_flat_norm, to check that our method worked. (In neural networks, we normalize values because it usually improves performance.)\n",
    "\n",
    "\n",
    "    data_flat_norm = data_flat/255\n",
    "    print(data_flat_norm[0])\n",
    "\n",
    "    The output from the above: \n",
    "[0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
    " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
    " 0.96862745 0.49803922 0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
    " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
    " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.19215686\n",
    " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
    " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
    " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
    " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
    " 0.96862745 0.94509804 0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
    " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.04313725\n",
    " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.1372549  0.94509804\n",
    " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
    " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
    " 0.58823529 0.10588235 0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
    " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.15294118 0.58039216\n",
    " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
    " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
    " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.07058824 0.67058824\n",
    " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
    " 0.31372549 0.03529412 0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
    " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.53333333 0.99215686\n",
    " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.        ]\n",
    "\n",
    "\n",
    "#### Splitting data into training and test data\n",
    "\n",
    "Split the arrays, data_flat_norm and target, into training data and testing data, with 20% (0.20) for testing and the remaining for training, using train_test_split function from sklearn.preprocessing module (NOTE: professor made a mistake, 'sklearn.preprocessing' should be 'sklearn.model_selection').\n",
    "\n",
    "#### Training MLPClassifier classifier from sklearn.neural_network module\n",
    "\n",
    "Train MLPClassifier classifier with the training data (X_train) and its corresponding label data (y_train) from the previous step using the following attribute values for MLPClassifier.\n",
    "\n",
    "    hidden_layer_sizes = (30, )\n",
    "    max_iter = 50\n",
    "    learning_rate_init = 0.05\n",
    "    verbose = 1\n",
    "    random_state = 1\n",
    "\n",
    "    \n",
    "#### Display score for training and testing values\n",
    "\n",
    "\n",
    "\n",
    "#### Display accuracy_score, classification report, and confusion_matrix\n",
    "\n",
    "    \n",
    "\n",
    "#### Experiments\n",
    "\n",
    "You are to do the following three experiments:\n",
    "\n",
    "__Experiment 1__\n",
    "\n",
    "Keep the following values fixed\n",
    "\n",
    "    max_iter = 50\n",
    "    learning_rate_init = 0.05\n",
    "\n",
    "Change first hidden laye sizes as below and record the corresponding accuracy score:\n",
    "\n",
    "    hidden layer sizes = (30,)    accuracy score = \n",
    "    hidden layer sizes = (50,)   accuracy score =\n",
    "    hidden layer sizes = (70,)   accuracy score = \n",
    "\n",
    "__Experiment 2__\n",
    "\n",
    "Keep the following values fixed\n",
    "\n",
    "    max_iter = 50\n",
    "    hidden layer sizes = (30,)\n",
    "\n",
    "Change the learning rate as below and record the corresponding accuracy score:\n",
    "\n",
    "    learning rate = 0.05   accuracy score = \n",
    "    learning rate = 0.1    accuracy score = \n",
    "\n",
    "__Experiment 3__\n",
    "\n",
    "Keep the following values fixed\n",
    "\n",
    "    hidden layer sizes = (30,)\n",
    "    learning rate = 0.05\n",
    "\n",
    "Change the max_iter as below and record the corresponding accuracy score:\n",
    "\n",
    "    max_iter = 25    accuracy score = \n",
    "    max_iter = 50   accuracy score = \n",
    "    max_iter = 75   accuracy score =\n",
    "\n",
    "__Summary Report__\n",
    "\n",
    "Write a short paragraph summarizing the above results and what you learn about AI from the above experiments,\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc1b45d",
   "metadata": {},
   "source": [
    "## Title: ANN Digit Classification Using MLPClassifier\n",
    "\n",
    "### Keith Yrisarri Stateson\n",
    "July 19, 2024. Python 3.11.0\n",
    "\n",
    "##### Summary\n",
    "This assignment involves building an Artificial Neural Network (ANN) to classify handwritten digits from the MNIST dataset using the MLPClassifier from Scikit-learn's neural_network module. The goal is to develop a model that can accurately recognize and classify images of digits (0-9). The assignment includes steps such as loading and preprocessing the data, building and training the model, evaluating its performance, and conducting experiments to optimize the model's parameters.\n",
    "\n",
    "Assumptions\n",
    "The dataset comprises images and their corresponding labels stored in images.pkl and targets.pkl. The input images are 28x28 pixels in size and need to be flattened for input into the neural network. Normalization of pixel values (0-255) to a range of 0-1 improves the performance of the neural network. The MLPClassifier is suitable for this classification task and can be optimized through experimentation with different parameters.\n",
    "\n",
    "##### Summary\n",
    "Data Loading and Preprocessing\n",
    "- Importing Libraries\n",
    "- Loading the Dataset\n",
    "- Exploring the Data\n",
    "- Reshaping Images\n",
    "- Normalizing Pixel Values\n",
    "- Splitting Data into Training and Test Sets\n",
    "\n",
    "Building the ANN Model\n",
    "- Model Architecture\n",
    "\n",
    "Training the Model\n",
    "- Training Process\n",
    "- Monitoring Training Performance\n",
    "- Handle overfitting\n",
    "\n",
    "Evaluating the Model\n",
    "- Model Evaluation Metrics\n",
    "- Accuracy and Loss Visualization\n",
    "- Confusion Matrix Analysis\n",
    "\n",
    "Experiments\n",
    "- Experiment 1: Adjusting Hidden Layer Sizes\n",
    "- Experiment 2: Adjusting Learning Rates\n",
    "- Experiment 3: Adjusting Maximum Iterations\n",
    "\n",
    "Conclusion\n",
    "- Summary of findings, insights, and observations\n",
    "- Future improvements and considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ef354",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1f81b9-3645-4888-9c7c-74208ea3cab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "f = open (\"images.pkl\", 'rb')\n",
    "images = pickle.load(f)\n",
    "f.close()\n",
    "f = open (\"target.pkl\", 'rb')\n",
    "target = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print (type (images))\n",
    "print (type (target))\n",
    "print(images.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c73e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print (images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960e99cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8]\n",
      "\n",
      "\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print (target)\n",
    "print('\\n')\n",
    "print (target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f4e579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10cdf76d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(images[0])\n",
    "# plt.imshow(images[0], cmap=plt.cm.gray_r, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a0b584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "data_flat = images.reshape(len(images),28*28)\n",
    "print (data_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc994605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "data_flat_norm = data_flat/255\n",
    "print(data_flat_norm[0])  # print(data_flat_norm[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ea4d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.05098039 0.18823529 0.51372549 0.83921569 0.82352941 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.15294118 0.37647059 0.61176471 0.89411765 0.99215686\n",
      " 0.99215686 0.99215686 0.76862745 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.19215686 0.64705882 0.8        0.97254902\n",
      " 0.99215686 0.94117647 0.81568627 0.57254902 0.49411765 0.99215686\n",
      " 0.40784314 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.29411765 0.99607843 0.94117647 0.96470588 0.30196078 0.02745098\n",
      " 0.         0.         0.49411765 0.99215686 0.27058824 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.29411765 0.99607843\n",
      " 0.47843137 0.47058824 0.         0.         0.         0.\n",
      " 0.81568627 0.89019608 0.04705882 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.17647059 0.         0.\n",
      " 0.         0.         0.         0.23921569 0.99607843 0.61176471\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.23529412 0.99215686 0.52156863 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.42745098\n",
      " 0.99215686 0.3372549  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.02352941 0.78039216 0.89411765 0.05490196\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.11764706 0.99215686 0.63921569 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.23921569 0.99607843\n",
      " 0.60784314 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.41176471 0.99215686 0.34901961 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.6        0.97647059 0.07058824 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.9254902  0.75686275\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.99607843 0.67058824 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.20784314\n",
      " 1.         0.47058824 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.29411765 0.99607843 0.31372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.29411765 0.99607843 0.47058824 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.29411765\n",
      " 0.99607843 0.61960784 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.20784314 0.96078431 0.15686275\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(data_flat_norm, target, test_size=0.2, random_state=0)\n",
    "\n",
    "print (X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f934a",
   "metadata": {},
   "source": [
    "## Build the ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dbdd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Artificial Neural Network (ANN) model\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf=MLPClassifier(hidden_layer_sizes=(30, ), max_iter=50, learning_rate_init=0.05, verbose=1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff7928",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0160476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38482498\n",
      "Iteration 2, loss = 0.26194635\n",
      "Iteration 3, loss = 0.24298655\n",
      "Iteration 4, loss = 0.22401414\n",
      "Iteration 5, loss = 0.22783768\n",
      "Iteration 6, loss = 0.22732241\n",
      "Iteration 7, loss = 0.22727139\n",
      "Iteration 8, loss = 0.21371720\n",
      "Iteration 9, loss = 0.21409649\n",
      "Iteration 10, loss = 0.21209832\n",
      "Iteration 11, loss = 0.22849224\n",
      "Iteration 12, loss = 0.22576485\n",
      "Iteration 13, loss = 0.22083767\n",
      "Iteration 14, loss = 0.21798569\n",
      "Iteration 15, loss = 0.21962274\n",
      "Iteration 16, loss = 0.21223700\n",
      "Iteration 17, loss = 0.22311094\n",
      "Iteration 18, loss = 0.21025982\n",
      "Iteration 19, loss = 0.21707725\n",
      "Iteration 20, loss = 0.22658597\n",
      "Iteration 21, loss = 0.21606773\n",
      "Iteration 22, loss = 0.21838172\n",
      "Iteration 23, loss = 0.20921266\n",
      "Iteration 24, loss = 0.21265455\n",
      "Iteration 25, loss = 0.21651203\n",
      "Iteration 26, loss = 0.22735869\n",
      "Iteration 27, loss = 0.22091869\n",
      "Iteration 28, loss = 0.19940928\n",
      "Iteration 29, loss = 0.21453394\n",
      "Iteration 30, loss = 0.22085885\n",
      "Iteration 31, loss = 0.22039183\n",
      "Iteration 32, loss = 0.22509706\n",
      "Iteration 33, loss = 0.22135356\n",
      "Iteration 34, loss = 0.22438214\n",
      "Iteration 35, loss = 0.21925105\n",
      "Iteration 36, loss = 0.22161808\n",
      "Iteration 37, loss = 0.23810503\n",
      "Iteration 38, loss = 0.21460896\n",
      "Iteration 39, loss = 0.22421463\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(30,), learning_rate_init=0.05, max_iter=50,\n",
       "              random_state=1, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(30,), learning_rate_init=0.05, max_iter=50,\n",
       "              random_state=1, verbose=1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(30,), learning_rate_init=0.05, max_iter=50,\n",
       "              random_state=1, verbose=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit (X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbac6e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444583333333333\n",
      "0.9291666666666667\n"
     ]
    }
   ],
   "source": [
    "# Scores range from 0 to 1, with a larger score indicating a better fit.\n",
    "# The score is computed as the fraction of correctly classified samples.\n",
    "\n",
    "# A difference between the training score and the test score is evidence of overfitting and can be reduced by decreasing the complexity of the model (e.g., the number of parameters) or by increasing the size of the training set. In this model, the difference is negligible.\n",
    "\n",
    "print(clf.score(X_train,y_train))\n",
    "print (clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca0b79",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2447505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9291666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      1205\n",
      "           1       0.98      0.97      0.98      1379\n",
      "           2       0.92      0.93      0.92      1166\n",
      "           3       0.98      0.88      0.92      1208\n",
      "           4       0.93      0.94      0.93      1153\n",
      "           5       0.92      0.93      0.92      1075\n",
      "           6       0.97      0.94      0.96      1190\n",
      "           7       0.99      0.90      0.94      1228\n",
      "           8       0.78      0.95      0.86      1191\n",
      "           9       0.88      0.92      0.90      1205\n",
      "\n",
      "    accuracy                           0.93     12000\n",
      "   macro avg       0.93      0.93      0.93     12000\n",
      "weighted avg       0.93      0.93      0.93     12000\n",
      "\n",
      "[[1132    0    1    0    5    6    8    0   53    0]\n",
      " [   0 1338    5    1    3    1    0    1   26    4]\n",
      " [   4    6 1079    6    9    0    7    1   53    1]\n",
      " [   0    2   43 1059    1   30    0    3   43   27]\n",
      " [   1    1    1    0 1079    0    7    3   28   33]\n",
      " [   6    3    3   14   10  997    4    0   30    8]\n",
      " [   5    1    0    0    7   29 1122    0   26    0]\n",
      " [   2    2   35    1    6    0    0 1101   11   70]\n",
      " [   2    8    6    1    9   19    5    0 1135    6]\n",
      " [   2    0    0    4   32    6    0    2   51 1108]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix (y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b59ce",
   "metadata": {},
   "source": [
    "## Experiment 1 - adjusting hidden_layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef73d053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with hidden layer sizes: (30,)\n",
      "CLF Score on training set: 0.9445\n",
      "CLF Score on test set: 0.9292\n",
      "Accuracy score: 0.9292\n",
      "--------------------------------------------------\n",
      "Experiment with hidden layer sizes: (50,)\n",
      "CLF Score on training set: 0.9601\n",
      "CLF Score on test set: 0.9397\n",
      "Accuracy score: 0.9397\n",
      "--------------------------------------------------\n",
      "Experiment with hidden layer sizes: (70,)\n",
      "CLF Score on training set: 0.9465\n",
      "CLF Score on test set: 0.9322\n",
      "Accuracy score: 0.9322\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Access the accuracy of the model by adjusting the hidden_layer_sizes parameter\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "hidden_layer_sizes = [(30,), (50,), (70,)]\n",
    "\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    clf=MLPClassifier(hidden_layer_sizes=hidden_layer_size, max_iter=50, learning_rate_init=0.05, verbose=0, random_state=1)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f'Experiment with hidden layer sizes: {hidden_layer_size}')\n",
    "    print(f'CLF Score on training set: {clf.score(X_train, y_train):.4f}')\n",
    "    print(f'CLF Score on test set: {clf.score(X_test, y_test):.4f}')\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f'Accuracy score: {accuracy_score(y_test, y_pred):.4f}')\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c9ce1e",
   "metadata": {},
   "source": [
    "## Experiment 2 - adjusting learning_rate_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b249921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with learning rate init: 0.05\n",
      "CLF Score on training set: 0.9445\n",
      "CLF Score on test set: 0.9292\n",
      "Accuracy score: 0.9292\n",
      "--------------------------------------------------\n",
      "Experiment with learning rate init: 0.1\n",
      "CLF Score on training set: 0.8856\n",
      "CLF Score on test set: 0.8729\n",
      "Accuracy score: 0.8729\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rate_init_list = [0.05, 0.1]\n",
    "\n",
    "for lri in learning_rate_init_list:\n",
    "    clf=MLPClassifier(hidden_layer_sizes=(30,), max_iter=50, learning_rate_init=lri, verbose=0, random_state=1)\n",
    "    \n",
    "    clf.fit (X_train,y_train)\n",
    "    print(f'Experiment with learning rate init: {lri}')\n",
    "    print(f'CLF Score on training set: {clf.score(X_train, y_train):.4f}')\n",
    "    print(f'CLF Score on test set: {clf.score(X_test, y_test):.4f}')\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f'Accuracy score: {accuracy_score(y_test, y_pred):.4f}')\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8929e",
   "metadata": {},
   "source": [
    "## Experiment 3 - adjusting max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7de1c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with max iter: 25\n",
      "CLF Score on training set: 0.9475\n",
      "CLF Score on test set: 0.9327\n",
      "Accuracy score: 0.9327\n",
      "--------------------------------------------------\n",
      "Experiment with max iter: 50\n",
      "CLF Score on training set: 0.9445\n",
      "CLF Score on test set: 0.9292\n",
      "Accuracy score: 0.9292\n",
      "--------------------------------------------------\n",
      "Experiment with max iter: 75\n",
      "CLF Score on training set: 0.9445\n",
      "CLF Score on test set: 0.9292\n",
      "Accuracy score: 0.9292\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "max_iter_list = [25, 50, 75]\n",
    "\n",
    "for mi in max_iter_list:\n",
    "    clf=MLPClassifier(hidden_layer_sizes=(30,), max_iter=mi, learning_rate_init=0.05, verbose=0, random_state=1)\n",
    "    \n",
    "    clf.fit (X_train,y_train)\n",
    "    print(f'Experiment with max iter: {mi}')\n",
    "    print(f'CLF Score on training set: {clf.score(X_train, y_train):.4f}')\n",
    "    print(f'CLF Score on test set: {clf.score(X_test, y_test):.4f}')\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f'Accuracy score: {accuracy_score(y_test, y_pred):.4f}')\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a8031",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c268231",
   "metadata": {},
   "source": [
    "he MLPClassifier class from skikit-learn library was used to build a neural network model for image classificaion. The model performance was optimized by adjusting the hyperparameters e.g., the number of hidden layers, the learning rate, and the number of iterations. Other hyperparameters can also be modified e.g. the number of neurons in each layer. The model performed well regarding overfitting of training and testing data, thus, no overfitting adjustments were needed.\n",
    "\n",
    "Metrics were conducted to assess model accuracy, which performed well. Having learned to use the MLPClassifier classifier to train a neural network model on the MNIST dataset, which contained images of handwritten digits, the model can now be applied to recognize handwritten digits with a high degree of accuracy.\n",
    "\n",
    "Best hyperparameter performers:\n",
    "- Hidden layer sizes: (50,)\n",
    "    - Accuracy score of 0.94\n",
    "- Learning rate: 0.05\n",
    "    - Accuracy score of 0.93\n",
    "- Max iter: 25\n",
    "    - Accuracy score of 0.93\n",
    "\n",
    "The MLPClassifier is a powerful tool for building nueral network models for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966a7ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
