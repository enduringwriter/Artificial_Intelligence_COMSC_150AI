{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf22276-e63e-401b-b1c0-6a1db481c693",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Neural networks using MLPClassifier from sklearn.neural_network module\n",
    "\n",
    "### Classifying (Recognizing) Hand-written Digits using neural networks\n",
    "\n",
    "### Objectives\n",
    "\n",
    "On completing this assignment, students will be able to write a simple AI application for classifying (Recognizing) hand-written digits using neural networks.\n",
    "\n",
    "### Readings for Neural Networks\n",
    "\n",
    "#### Required Reading: Introduction\n",
    "\n",
    "https://www.ibm.com/topics/neural-networks#:~:text=One%20of%20the%20best%2Dknown,heart%20of%20deep%20learning%20models.\n",
    "\n",
    "#### Optional Reading: Backpropagation\n",
    "\n",
    "https://www.geeksforgeeks.org/backpropagation-in-neural-network\n",
    "\n",
    "#### Required Reading: Code Example\n",
    "\n",
    "https://www.pluralsight.com/resources/blog/guides/machine-learning-neural-networks-scikit-learn#:~:text=In%20this%20guide%2C%20you%20have,training%20and%20test%20data%2C%20respectivetiv\n",
    "\n",
    "### Additionally do the following\n",
    "\n",
    "For training and testing the application, we use MLPClassifier classifier from sklearn.neural_network module as shown in the code fragment below. \n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "    clf=MLPClassifier(hidden_layer_sizes=(64, 10), max_iter=200, learning_rate_init=0.05, verbose=1, random_state=1)\n",
    "    \n",
    "The above MLPClassifier classifier will employs neural networks during training. In neural networks, the input layer size is always the same as the number of inputs and the output layer size is always the same as the number of outputs. \n",
    "\n",
    "In this case, since each digit is represented by 64 (8 by 8) pixel values, the input layer size will be 64. Since digits are to be classified into 10 different numerals from 0 to 9, the output layer size will be 10.\n",
    "\n",
    "We chose, two hidden layers as indicated in the code fragment above (64,10). We chose the first hidden layer size to be 64 (the same as the input layer size) and the second hidden layer size to be 10 (the same as the output layer size). \n",
    "\n",
    "We chose maximum iteration to be 200 as shown in code fragment above. So, during training, the classifier (clf) will run the same data 200 times. It will randomly select its internal weight parameter values at the beginning the of first run and then adjust these values at the end of each run in an attempt to reduce the loss (error) (the difference between the actual output values and the true values) for the next run. \n",
    "\n",
    "We chose learning rate to be 0.05 in the code fragment above. So, the classifier will multiply the loss value with 0.05 to decide on how much to try to reduce the loss for the next run. With a smaller learning rate (such as 0.01), it will try to reduce loss by smaller amount and for a larger learning rate (such as 0.1), it will try to reduce the loss by a smaller amount at each run in an attempt to get a loss approaching zero.\n",
    "\n",
    "We chose verbose to be 1 (or True) in the above code fragment in order to initiate printing of details such as the loss value after each run.\n",
    "\n",
    "We chose to specify the value for random_state in the above code fragment, to ensure that the classifier would start from the same random state at each execution of the application and its behavior will be consistent at each run of the application. This will allow us to experiment with different parameter values while keeping the classifier behavior consistent.\n",
    "\n",
    "#### Experiments\n",
    "   \n",
    "You are to do the following three experiments:\n",
    "\n",
    "__Experiment 1__\n",
    "\n",
    "Keep the following values fixed\n",
    "\n",
    "    max_iter = 200\n",
    "    learning_rate_init = 0.05\n",
    "\n",
    "Change the hidden layer sizes as below and record the corresponding accuracy score:\n",
    "\n",
    "    hidden_layer_sizes = (50,10)    accuracy score =\n",
    "    hidden_layer_sizes = (100,10)   accuracy score =\n",
    "    hidden_layer_sizes = (200,10)   accuracy score = \n",
    "\n",
    "__Experiment 2__\n",
    "\n",
    "Keep the following values fixed\n",
    "\n",
    "    max_iter = 200\n",
    "    hidden_layer_size = (100,10)\n",
    "\n",
    "Change the learning rate as below and record the corresponding accuracy score:\n",
    "\n",
    "    learning_rate_init = 0.01   accuracy score =\n",
    "    learning_rate_init = 0.05   accuracy score =\n",
    "    learning_rate_init = 0.1    accuracy score = \n",
    "\n",
    "__Experiment 3__\n",
    "\n",
    "Keep the following values fixed\n",
    "\n",
    "    hidden_layer_sizes = (100,10)\n",
    "    learning_rate_init = 0.05\n",
    "\n",
    "Change the max_iter as below and record the corresponding accuracy score:\n",
    "\n",
    "    max_iter = 50    accuracy score = \n",
    "    max_iter = 100   accuracy score = \n",
    "    max_iter = 200   accuracy score =\n",
    "\n",
    "__Summary Report__\n",
    "\n",
    "Write a short paragraph summarizing the above results and what you learn about AI from the above experiment.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37f8b326-cc82-49f1-ad21-7ddd641bf53d",
   "metadata": {},
   "source": [
    "Load digit data set from sklearn's datasets module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2f9c968f-a5fd-437e-af4b-282f9564596b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "digits=datasets.load_digits()\n",
    "dir (digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e13e28d-a8eb-414b-a96b-151da083afdd",
   "metadata": {},
   "source": [
    "from digit data set extract data, images, and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2dc09dd6-d195-4e3f-9240-9bcebdaaa721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(1797, 64)\n",
      "(1797, 8, 8)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "data = digits.data\n",
    "images = digits.images\n",
    "target = digits.target\n",
    "print (type (data))\n",
    "print (type (images))\n",
    "print (type (target))\n",
    "print (data.shape)\n",
    "print (images.shape)\n",
    "print (target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb234133-04cc-42d3-ada9-6f2b8dc7ea13",
   "metadata": {},
   "source": [
    "Display first element of data, images, and target arrays and show the first element of images as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d873ce39-73f7-461a-b496-2b816376a022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4e8858f880>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print (data[0])\n",
    "print (images[0])\n",
    "print (target[0])\n",
    "plt.matshow(images[0])\n",
    "#plt.imshow(images[0], cmap=plt.cm.gray_r, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d082beed-3ddc-4c2f-ba1c-88f43d496e01",
   "metadata": {},
   "source": [
    "So, from the above, it is clear that the elements of data, images, and target arrays carry data about images in different form and each element of the arrays contains data about one image as indicated below:\n",
    "\n",
    "- An element of data array contains 64 pixel values in one dimension array \n",
    "- An element of images array contains the same pixel values as an 8 by 8 two dimensional array (So, we were able to show it in image form using  plt)\n",
    "- An element of target contains the information about the image as an int\n",
    "\n",
    "We will leave images array as it is and simply use it for displaying images.\n",
    "\n",
    "We will use data array for processing. Since, the pixel values vary form 0 (black) to 255 (black), we will normalize them (bring them in the range from 0 to 1) by dividing each by 255. Since, it is a numpy array, we can divide each value in the whole array in a single step and store it in a variable data_flat_norm (the name indicates that the array elements are one dimensional (or flat) and they are normalized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1b5036d6-7bb4-475f-8fa1-b94d7e6fa515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.01960784, 0.05098039, 0.03529412,\n",
       "        0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05098039, 0.05882353, 0.03921569, 0.05882353, 0.01960784,\n",
       "        0.        , 0.        , 0.01176471, 0.05882353, 0.00784314,\n",
       "        0.        , 0.04313725, 0.03137255, 0.        , 0.        ,\n",
       "        0.01568627, 0.04705882, 0.        , 0.        , 0.03137255,\n",
       "        0.03137255, 0.        , 0.        , 0.01960784, 0.03137255,\n",
       "        0.        , 0.        , 0.03529412, 0.03137255, 0.        ,\n",
       "        0.        , 0.01568627, 0.04313725, 0.        , 0.00392157,\n",
       "        0.04705882, 0.02745098, 0.        , 0.        , 0.00784314,\n",
       "        0.05490196, 0.01960784, 0.03921569, 0.04705882, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02352941, 0.05098039,\n",
       "        0.03921569, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.04705882, 0.05098039,\n",
       "        0.01960784, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313725, 0.0627451 , 0.03529412, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.01176471, 0.05882353,\n",
       "        0.0627451 , 0.02352941, 0.        , 0.        , 0.        ,\n",
       "        0.02745098, 0.05882353, 0.0627451 , 0.0627451 , 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "        0.0627451 , 0.0627451 , 0.01176471, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00392157, 0.0627451 , 0.0627451 ,\n",
       "        0.02352941, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00392157, 0.0627451 , 0.0627451 , 0.02352941, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.04313725,\n",
       "        0.0627451 , 0.03921569, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.01568627, 0.05882353,\n",
       "        0.04705882, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01176471, 0.0627451 , 0.05882353, 0.05490196, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.03137255, 0.05098039,\n",
       "        0.03137255, 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00392157, 0.02352941, 0.05882353, 0.04313725,\n",
       "        0.        , 0.        , 0.        , 0.00392157, 0.03137255,\n",
       "        0.05098039, 0.05882353, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.03529412, 0.0627451 , 0.0627451 , 0.01960784,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01176471,\n",
       "        0.05098039, 0.0627451 , 0.0627451 , 0.04313725, 0.01960784,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01176471,\n",
       "        0.04313725, 0.0627451 , 0.03529412, 0.        ]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_flat_norm = data/255\n",
    "data_flat_norm[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ec6e2-fae4-4a50-bb49-761a85b825a3",
   "metadata": {},
   "source": [
    "Split the data in data_flat_norm into training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "efbe6467-40be-473e-b4a8-ff7e905e1b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.03529412 0.05882353 0.00784314\n",
      " 0.         0.         0.         0.         0.01960784 0.0627451\n",
      " 0.04313725 0.00392157 0.         0.         0.         0.\n",
      " 0.05098039 0.05882353 0.00392157 0.         0.         0.\n",
      " 0.         0.00784314 0.0627451  0.04313725 0.         0.\n",
      " 0.         0.         0.         0.00784314 0.0627451  0.04313725\n",
      " 0.01568627 0.01568627 0.         0.         0.         0.00784314\n",
      " 0.05882353 0.0627451  0.0627451  0.05490196 0.03921569 0.00392157\n",
      " 0.         0.         0.03529412 0.0627451  0.02745098 0.01176471\n",
      " 0.05882353 0.02352941 0.         0.         0.         0.02745098\n",
      " 0.05882353 0.0627451  0.0627451  0.02352941]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(data_flat_norm, target, test_size=0.2, random_state=0)\n",
    "\n",
    "print (X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc8423-5b23-496b-9e67-8cc9561b7d2d",
   "metadata": {},
   "source": [
    "Create MLPClassifier classifier for neural network training and set its parameters as below.\n",
    "\n",
    "- hidden_layer_sizes is used to indicate the number of hidden layers and the number of computing units in each hidden layer\n",
    "- max_iter is used to indicate the number of times to do the training with the same test data, each time changing the internal weight parameter values in the direction of reducing the loss.\n",
    "- verbose A value of True or 1 request that the details such as loss values be displayed after each run. \n",
    "-random_state By specifying the same random_state value each time you run the classifier, you are ensuring that the same sequence of events will occur each time. This can be helpful in debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5f0e219c-a8f0-437d-af72-af80114dffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf=MLPClassifier(hidden_layer_sizes=(64, 10), max_iter=200, learning_rate_init=0.05, verbose=1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267131c3-8c9c-4bd7-b88c-e80ba08b8218",
   "metadata": {},
   "source": [
    "Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "aabe0ddf-6489-4073-af62-fd9b5063da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30421014\n",
      "Iteration 2, loss = 2.10670810\n",
      "Iteration 3, loss = 1.84612755\n",
      "Iteration 4, loss = 1.47339401\n",
      "Iteration 5, loss = 1.10939035\n",
      "Iteration 6, loss = 0.93026683\n",
      "Iteration 7, loss = 0.93243884\n",
      "Iteration 8, loss = 0.81634669\n",
      "Iteration 9, loss = 0.75415133\n",
      "Iteration 10, loss = 0.68509135\n",
      "Iteration 11, loss = 0.67650182\n",
      "Iteration 12, loss = 0.61864945\n",
      "Iteration 13, loss = 0.60401945\n",
      "Iteration 14, loss = 0.58176095\n",
      "Iteration 15, loss = 0.59163216\n",
      "Iteration 16, loss = 0.51643214\n",
      "Iteration 17, loss = 0.49754356\n",
      "Iteration 18, loss = 0.47375272\n",
      "Iteration 19, loss = 0.47018774\n",
      "Iteration 20, loss = 0.47247387\n",
      "Iteration 21, loss = 0.41392364\n",
      "Iteration 22, loss = 0.41937124\n",
      "Iteration 23, loss = 0.39347308\n",
      "Iteration 24, loss = 0.36090958\n",
      "Iteration 25, loss = 0.36226367\n",
      "Iteration 26, loss = 0.39116833\n",
      "Iteration 27, loss = 0.35710670\n",
      "Iteration 28, loss = 0.36513030\n",
      "Iteration 29, loss = 0.34982524\n",
      "Iteration 30, loss = 0.35746274\n",
      "Iteration 31, loss = 0.33690817\n",
      "Iteration 32, loss = 0.30263923\n",
      "Iteration 33, loss = 0.29332494\n",
      "Iteration 34, loss = 0.29103495\n",
      "Iteration 35, loss = 0.27839259\n",
      "Iteration 36, loss = 0.26509278\n",
      "Iteration 37, loss = 0.26110502\n",
      "Iteration 38, loss = 0.25410178\n",
      "Iteration 39, loss = 0.26250161\n",
      "Iteration 40, loss = 0.24354357\n",
      "Iteration 41, loss = 0.23368341\n",
      "Iteration 42, loss = 0.23235105\n",
      "Iteration 43, loss = 0.23812641\n",
      "Iteration 44, loss = 0.23011445\n",
      "Iteration 45, loss = 0.25472374\n",
      "Iteration 46, loss = 0.27219303\n",
      "Iteration 47, loss = 0.23973114\n",
      "Iteration 48, loss = 0.20963630\n",
      "Iteration 49, loss = 0.21442351\n",
      "Iteration 50, loss = 0.21048664\n",
      "Iteration 51, loss = 0.20586002\n",
      "Iteration 52, loss = 0.21878596\n",
      "Iteration 53, loss = 0.21258287\n",
      "Iteration 54, loss = 0.20756431\n",
      "Iteration 55, loss = 0.19348276\n",
      "Iteration 56, loss = 0.21613260\n",
      "Iteration 57, loss = 0.19723256\n",
      "Iteration 58, loss = 0.20219723\n",
      "Iteration 59, loss = 0.18738906\n",
      "Iteration 60, loss = 0.18384767\n",
      "Iteration 61, loss = 0.19299826\n",
      "Iteration 62, loss = 0.22541226\n",
      "Iteration 63, loss = 0.21843696\n",
      "Iteration 64, loss = 0.19314920\n",
      "Iteration 65, loss = 0.21712278\n",
      "Iteration 66, loss = 0.18458654\n",
      "Iteration 67, loss = 0.16509418\n",
      "Iteration 68, loss = 0.18853874\n",
      "Iteration 69, loss = 0.21406324\n",
      "Iteration 70, loss = 0.16938074\n",
      "Iteration 71, loss = 0.16809055\n",
      "Iteration 72, loss = 0.15719367\n",
      "Iteration 73, loss = 0.15222753\n",
      "Iteration 74, loss = 0.15304945\n",
      "Iteration 75, loss = 0.16697205\n",
      "Iteration 76, loss = 0.15368769\n",
      "Iteration 77, loss = 0.14523191\n",
      "Iteration 78, loss = 0.15744889\n",
      "Iteration 79, loss = 0.13966181\n",
      "Iteration 80, loss = 0.21448747\n",
      "Iteration 81, loss = 0.17154046\n",
      "Iteration 82, loss = 0.14129746\n",
      "Iteration 83, loss = 0.15368842\n",
      "Iteration 84, loss = 0.15626162\n",
      "Iteration 85, loss = 0.14026644\n",
      "Iteration 86, loss = 0.12650421\n",
      "Iteration 87, loss = 0.13193079\n",
      "Iteration 88, loss = 0.12452374\n",
      "Iteration 89, loss = 0.12457683\n",
      "Iteration 90, loss = 0.13056548\n",
      "Iteration 91, loss = 0.13276045\n",
      "Iteration 92, loss = 0.12244382\n",
      "Iteration 93, loss = 0.11999311\n",
      "Iteration 94, loss = 0.10944904\n",
      "Iteration 95, loss = 0.10867957\n",
      "Iteration 96, loss = 0.11112730\n",
      "Iteration 97, loss = 0.12196614\n",
      "Iteration 98, loss = 0.11893110\n",
      "Iteration 99, loss = 0.11006301\n",
      "Iteration 100, loss = 0.10337204\n",
      "Iteration 101, loss = 0.11263667\n",
      "Iteration 102, loss = 0.10970136\n",
      "Iteration 103, loss = 0.09837782\n",
      "Iteration 104, loss = 0.09785997\n",
      "Iteration 105, loss = 0.10253246\n",
      "Iteration 106, loss = 0.10002161\n",
      "Iteration 107, loss = 0.10066076\n",
      "Iteration 108, loss = 0.09834383\n",
      "Iteration 109, loss = 0.08732736\n",
      "Iteration 110, loss = 0.09979496\n",
      "Iteration 111, loss = 0.12233312\n",
      "Iteration 112, loss = 0.10338355\n",
      "Iteration 113, loss = 0.12392667\n",
      "Iteration 114, loss = 0.10580012\n",
      "Iteration 115, loss = 0.10609026\n",
      "Iteration 116, loss = 0.09303700\n",
      "Iteration 117, loss = 0.08835531\n",
      "Iteration 118, loss = 0.07998605\n",
      "Iteration 119, loss = 0.07735666\n",
      "Iteration 120, loss = 0.08165152\n",
      "Iteration 121, loss = 0.08314675\n",
      "Iteration 122, loss = 0.07998023\n",
      "Iteration 123, loss = 0.08512707\n",
      "Iteration 124, loss = 0.07858185\n",
      "Iteration 125, loss = 0.07262822\n",
      "Iteration 126, loss = 0.08276202\n",
      "Iteration 127, loss = 0.07182444\n",
      "Iteration 128, loss = 0.08378248\n",
      "Iteration 129, loss = 0.08722164\n",
      "Iteration 130, loss = 0.09429688\n",
      "Iteration 131, loss = 0.09098369\n",
      "Iteration 132, loss = 0.07680781\n",
      "Iteration 133, loss = 0.08897667\n",
      "Iteration 134, loss = 0.07502823\n",
      "Iteration 135, loss = 0.06419847\n",
      "Iteration 136, loss = 0.07284343\n",
      "Iteration 137, loss = 0.08037793\n",
      "Iteration 138, loss = 0.09562804\n",
      "Iteration 139, loss = 0.08675762\n",
      "Iteration 140, loss = 0.08748264\n",
      "Iteration 141, loss = 0.09563598\n",
      "Iteration 142, loss = 0.11499750\n",
      "Iteration 143, loss = 0.10216831\n",
      "Iteration 144, loss = 0.09377897\n",
      "Iteration 145, loss = 0.11400445\n",
      "Iteration 146, loss = 0.09701196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(64, 10), learning_rate_init=0.05,\n",
       "              random_state=1, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(64, 10), learning_rate_init=0.05,\n",
       "              random_state=1, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(64, 10), learning_rate_init=0.05,\n",
       "              random_state=1, verbose=1)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit (X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace7c48-b154-4674-a30c-90c91021e3cb",
   "metadata": {},
   "source": [
    "Since, the loss (error) (the difference between the true and predicted value) may not be zero at the end of training, calculate the classifier's score with the training data. Also, calculate its score with the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "09976558-6985-4902-9719-5f4b390a4800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9679888656924147\n",
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(X_train,y_train))\n",
    "print (clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "431da16b-441c-4039-af15-082987c60b7b",
   "metadata": {},
   "source": [
    "Also, print the accuracy_score, classification_report, and confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d06840e4-3108-431f-b9f9-2efa5c9756d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8888888888888888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       0.84      0.91      0.88        35\n",
      "           2       0.97      0.92      0.94        36\n",
      "           3       0.90      0.90      0.90        29\n",
      "           4       0.88      1.00      0.94        30\n",
      "           5       0.97      0.82      0.89        40\n",
      "           6       0.98      0.98      0.98        44\n",
      "           7       0.97      0.79      0.87        39\n",
      "           8       0.68      0.87      0.76        39\n",
      "           9       0.82      0.76      0.78        41\n",
      "\n",
      "    accuracy                           0.89       360\n",
      "   macro avg       0.90      0.90      0.89       360\n",
      "weighted avg       0.90      0.89      0.89       360\n",
      "\n",
      "[[27  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 32  0  0  0  0  1  0  2  0]\n",
      " [ 0  1 33  1  0  0  0  0  1  0]\n",
      " [ 0  0  1 26  0  0  0  0  2  0]\n",
      " [ 0  0  0  0 30  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 33  0  0  1  6]\n",
      " [ 0  1  0  0  0  0 43  0  0  0]\n",
      " [ 0  0  0  0  4  0  0 31  3  1]\n",
      " [ 0  4  0  1  0  0  0  0 34  0]\n",
      " [ 0  0  0  1  0  1  0  1  7 31]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, \\\n",
    "classification_report,confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix (y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed921ded-9980-4a38-b8a8-ab2ae09cb807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
