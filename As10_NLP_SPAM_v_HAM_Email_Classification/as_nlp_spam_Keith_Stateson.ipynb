{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504084ed-1876-430c-ac89-0ff58c9a5ae4",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "## Text Classification \n",
    "\n",
    "## Predicting Text Category of Email (Ham or Spam)\n",
    "\n",
    "## Objectives\n",
    "\n",
    "On completing the assignment, you will be able to write a simple ai application to classify emails into spam or ham (not spam).\n",
    "\n",
    "## Description\n",
    " \n",
    "Write an AI application which will classify emails into spam and ham (not spam). For training and testing purposes, please use the dataset provided in the file, ham_spam.csv. The dataset contains 5572 sample emails labeled as ham or spam. Use 80% of the items for training, and the remaining 20% for testing. Use RandomForestClassifier classifier of sklearn.ensemble module with initial parameters, n_estimators=500, random_state=0 when creating the classifier object. After testing, produce accuracy score, classification report, and confusion matrix. Then, try out a few of your own created short emails and note the predicted response.\n",
    "\n",
    "### Additionally, do the following:\n",
    "\n",
    "#### Training model \n",
    "\n",
    "Train the following classifiication models of sklearn's library on the same data as described above and print their performance using the values of accuracy score.\n",
    "\n",
    "- RandomForestClassifier\n",
    "- KNeighborsClassifier\n",
    "- LogisticRegression\n",
    "\n",
    "#### Individual Values\n",
    "Also, try 3 made up values of emails with the best performing model and print the result.\n",
    "\n",
    "\n",
    "#### Code for using different classification model\n",
    "\n",
    "Below, X_train and y_train contain training data features and their corresponding labels respectively and X_test contains the testing data.\n",
    "\n",
    "RandomForestClassifier\n",
    "\n",
    "    #Train\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=500)\n",
    "    clfl.fit (X_train,y_train)\n",
    "\n",
    "    #Test\n",
    "    y_prdict = clf.predict(X_test)\n",
    "\n",
    "KNeighborsClassifier\n",
    "\n",
    "    #Train\n",
    "    from sklearn.neighbors  import KNeighborsClassifier\n",
    "    clf = KNeighborsClassifier(n_neighbors= 5)\n",
    "    clf.fit (X_train,y_train)\n",
    "\n",
    "    #Test\n",
    "    y_prdict = clf.predict(X_test)\n",
    "\n",
    "LogisticRegression\n",
    "\n",
    "    #Train\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression ()\n",
    "    clfl.fit (X_train,y_train)\n",
    "\n",
    "    # Test\n",
    "    y_prdict = clf.predict(X_test)\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Follow the step below\n",
    "\n",
    "### Data set (corpus) used for the application\n",
    "\n",
    "Use the data set (corpus) in the following file:\n",
    "\n",
    "\"ham_spam.csv\"\n",
    "\n",
    "### Load the data set\n",
    "\n",
    "Load the testing data as below\n",
    "\n",
    "```\n",
    "df=pd.read_csv(\"ham_spam.csv\")\n",
    "```\n",
    "\n",
    "### Assign Category column to y and Message column to x\n",
    "\n",
    "- Assign the Category column to variable y (the labels).\n",
    "- Assign the Message column to variable X (the features). \n",
    "\n",
    "### Clean up text in X\n",
    "\n",
    "- remove all special characters (remove all characters except alphabets, digits, and space characters). Short words such as 'it', 'the' etc. (stopwords) will be removed during vectorization. \n",
    "\n",
    "\n",
    "### Vectorize \n",
    "\n",
    "\n",
    "- Vectorize X by using TfidfVectorizer vectorizer of sklearn.feature_extraction.text module with parameters below and call it X_list_vectorized:\n",
    "  \n",
    "max_features=2000, min_df=5,  max_df=0.7, stop_words=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "### Split data\n",
    "\n",
    "Split data X_vectorized, y into X_train, X_test, y_train, and y_test data using train_test_split function of sklearn.model_selection module. Use 80% of the data for training and the rest for testing.\n",
    "\n",
    "### Train algorithm\n",
    "\n",
    "Train RandomForestClassifier classifier (initial parameter: n_estimators=500) of sklearn.ensemble module using the training data X_train and y_train\n",
    "\n",
    "### Test Algorithm\n",
    "\n",
    "Test the trained classifier with testing data X_test\n",
    "\n",
    "### Print Result\n",
    "\n",
    "Print accuracy score, classification report, and confusion matrix\n",
    "\n",
    "### Test Short Emails\n",
    "\n",
    "Test a few made-up emails using the trained classifier and print the results.\n",
    "\n",
    "\n",
    "## Submittal\n",
    "\n",
    "The uploaded submittal should contain the following:\n",
    "\n",
    "- jpynb file after runninng the application from start to finish containing the marked source code, output, and your interaction.\n",
    "- the corresponding html file.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "In natural language problems, data is made up of sample documents (instead of sample values). Together these documents are called corpus (instead of data set). Since, our learning algorithms are based on numerical values, the documents in the corpus need to be encoded into numerical values (or numerical vectors) before using any algorithm.  \n",
    "\n",
    "#### Preprocessing\n",
    "\n",
    "Many words in the documents are not relevant to classifying the documents and can be excluded. In general, punctuation marks and other symbols are removed from the documents. Short words such as \"to\", \"on\", \"the\" etc. (called stop words) are also taken out. Furthermore, words with the same root such as eats, eating, ate, eaten, etc. are replaced with their common root words. So, documents go through a good deal of preprocessing before they are encoded into numerical values (numerical vectors). \n",
    "\n",
    "### Vectorizing Documents (Encoding documents into numerical values)\n",
    "\n",
    "The process of encoding documents into numerical values is called vectorizing because each document is encoded into a numerical vector (array of numerical values). The two common methods of vectorizing documents are \"Bag of Words\" (BOG) and \"Term Frequency Inverse Document Frequency\" (TFIDF) and they are described below.\n",
    "\n",
    "#### Bag of Words (BOW)\n",
    "\n",
    "In this method, each sample document is encoded into a numerical vector (numerical array) made up of several numerical values. \n",
    "\n",
    "##### Preparing vocabulary for the corpus\n",
    "\n",
    "In using this method, at first, we prepare vocabulary of all words used in the whole corpus (in all the sample documents in the data set) and assign each word a unique id (index) so that each word can be identified by its id (index). For example, if there are 200 different words used in the whole corpus, then its vocabulary is 200 words and each word is assigned a unique id (index) from 0 to 199.\n",
    "\n",
    "After that, we  assign to each document, a document vector (an array of numbers) of the same size as vocabulary size for the whole corpus. So, for a corpus with vocabulary size of 200 words, we assign each document, a 200 size numerical vector (numerical array) where the first value in the vector pertains to the word whose id (index) is 0, the second pertains to the word whose id (index) is 1, the third pertains to the word whose id (index) is 2, and so on. \n",
    "\n",
    "Then we start assigning values to vectors. In assigning values to a document vector, we start with the first value in the vector. This value pertains to the word whose id (index) is zero. So, in vocabulary, we lookup the word whose id (index) is zero. Then we go to the document and determine the frequency of use of that word (how many times this words has been used in the document). Then, we assign the frequency of use value as the value in the vector. (Note that if the word is never used in the document, its frequency of use is zero; if it is used once, its frequency of use is 1; if it is used twice, its frequency of use is 2;  and so on.) \n",
    "\n",
    "We repeat this process for every value in the vector and each time, we assign the frequency of use of the corresponding word in the document, as the value in the vector. Thus, each value in the document vector indicates the frequency of use of the corresponding word in the document. \n",
    "\n",
    "As an example of the above, see Example 1 below. In Example 1, our corpus is made up of three short sample documents, doc 1, doc 2, and doc 3, each containing a sentence. \n",
    "\n",
    "First we determine the vocabulary for the whole corpus. It comprises 8 words and is shown below. The ids (indices) of these words are also shown below and they are from 0 to 7.\n",
    "\n",
    "Then, we determine the vector for each document. \n",
    "\n",
    "For example, for doc 3, the first value of the vector pertains to the word whose id (index) is 0. From vocabulary, we find that the word with id (index) 0 is 'sue'. Then, we determine the frequency of use of the word 'sue' in doc 3. It is not used at all. Consequently, its frequency of use is zero. So, we assign 0 as the first value of the vector. \n",
    "\n",
    "Similarly, we determine the next value of the vector. The next value of the vector pertains to the word whose id (index) is 1. From vocabulary, we find that the word with id (index) 1 is 'is'. Then, we determine the frequency of use of the word 'is' in doc 3. The word 'is' is used in the document twice. Consequently, its frequency of use is 2. So, we assign 2 as the next value of the vector. \n",
    "\n",
    "We repeat this for determining other values of doc 3 vector. When all values are determined, the doc 3 vector values are: 0, 2, 2, 0, 1, 1, 1, 1 as shown below.\n",
    "\n",
    "##### Example 1\n",
    "\n",
    "              doc 1: sue is ok \n",
    "              doc 2: jim is ok \n",
    "              doc 3: sam is ok but joe is not ok \n",
    "\n",
    "              vocabulary:  sue is ok jim sam but joe not \n",
    "              word id's:    0   1  2  3   4   5   6   7 \n",
    "\n",
    "              doc 1 vector: 1   1  1  0   0   0   0   0 \n",
    "              doc 2 vector: 0   1  1  1   0   0   0   0 \n",
    "              doc 3 vector: 0   2  2  0   1   1   1   1 \n",
    "\n",
    "\n",
    "#### Term Frequency Inverse Document Frequency (TFIDF)\n",
    "\n",
    "This method of assigning a numerical vector to each document is identical to the method of 'Bag of Words' (BOW) described above except that, in the last step of assigning values to the vector, instead of assigning frequency of use values, we assign TFIDF values of the corresponding words.\n",
    "\n",
    "A TFIDF value of a word is calculated by multiplying its TF and IDF values as described below.\n",
    "\n",
    "##### Term Frequency (TF) value\n",
    "\n",
    "Term frequency of a word (TF) is equal to: (\"the frequency of the word in the document\" divided by \"the total number of words in the document\"). The concept behind TF is that the more frequent a word is in a document, the more it is relevant to the document. \n",
    "\n",
    "##### Inverse Document Frequency (IDF)\n",
    "\n",
    "Inverse Document Frequency (IDF) of a word is equal to: the log of (\"total number of documents in the corpus\" divided by \"the number of documents in which the word is used\"). When the corpus contains huge number of documents, the numerator and the quotient of the value in parentheses above can become very large. By taking a log of the value, the value of IDF is kept manageable. The concept behind IDF is that a word which is used in too many documents, such as the word 'the', that word is not relevant to the document. However, if a word is only used in a few documents, then it is relevant to those documents.  \n",
    "\n",
    "##### Combining TF and IDF\n",
    "\n",
    "TFIDF is obtained by multiplying TF and IDF. However, there is a variety of ways in which IDF and TFIDF are calculated and combined.\n",
    "\n",
    "#### An example of assigning TFIDF values \n",
    "\n",
    "For an example of assigning TFIDF values, see Example 2 below. Example 2 is identical to Example 1 above except that we are assigning TFIDF values to the document vectors in place of assigning the frequency of use values. \n",
    "\n",
    "The TFIDF vector values for the three documents doc 1, doc 2, and doc 3, are shown below. We describe below the process of determining values for doc 3 vector. \n",
    "\n",
    "The first value in the doc 3 vector pertains to the word whose id (index) is 0. From vocabulary, we find that the word with id (index) 0 is 'sue'. \n",
    "\n",
    "Since, word 'sue' is used 0 times (not used at all) in doc 3 out of a total of 8 words that make up 3, its TF value is 0 as shown below.\n",
    "\n",
    "TF (relative frequency of use) value for word 'sue' for doc 3 vector:\n",
    "frequency of use in doc 3 / total number of words in doc 3 = 0/8 = 0 \n",
    "\n",
    "TFIDF value for word 'sue': TF * IDF = 0 * IDF = 0\n",
    "\n",
    "Calculating in the same way, the first four values for doc 3 vector are 0.\n",
    "\n",
    "Now, we discuss the calculation for the fifth value of doc 3 vector. This value corresponds to the word whose id (index) is 4. Per vocabulary, that word is 'sam'. The calculation TFID for the word 'sam' for doc 3 vector are shown below.\n",
    "\n",
    "TF (relative frequency of use) value for word 'sam' in doc 3 vector:\n",
    "frequency of use in doc 3 / total number of words in doc 3 = 1/8 = 0.125 \n",
    "\n",
    "IDF (relative inverse document frequency) value for word 'sam' in doc 3 vector:\n",
    "log (total documents in the copus/number of documents containing 'sam')= \n",
    "log (3/1) = log 3 = 0.477 \n",
    "\n",
    "TFIDF value for word 'sam': TF * IDF = 0.125 * 0.477 = 0.06\n",
    "\n",
    "Similarly, the remaining values of doc 3 vector are .06 as indicated below.\n",
    "\n",
    "\n",
    "##### Example 2\n",
    "\n",
    "              doc 1: sue is ok \n",
    "              doc 2: jim is ok \n",
    "              doc 3: sam is ok but joe is not ok \n",
    "\n",
    "              vocabulary:   sue  is  ok  jim  sam  but  joe  not \n",
    "              word id's:    0    1   2   3    4    5    6    7 \n",
    "\n",
    "              doc 1 vector: .16  0   0   0    0    0    0    0 \n",
    "              doc 2 vector: 0    0   0  .16   0    0    0    0 \n",
    "              doc 3 vector: 0    0   0   0   .06  .06  .06  .06\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8385dd-3790-4a8c-a40a-5b34e2a969fa",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e8e74",
   "metadata": {},
   "source": [
    "## Title: NLP Assignment: Spam vs Ham Email Classification\n",
    "\n",
    "### Keith Yrisarri Stateson\n",
    "July 17, 2024. Python 3.11.0\n",
    "\n",
    "##### Summary\n",
    "This program is an AI application designed to classify emails into spam and ham (not spam) categories using supervised learning techniques. The goal is to develop a robust model that can accurately distinguish between spam and legitimate emails, enhancing email filtering systems. The program employs various machine learning algorithms, including RandomForestClassifier, KNeighborsClassifier, and LogisticRegression, to evaluate their performance on the provided email dataset.\n",
    "\n",
    "The assignment involves data cleaning, text vectorization using TF-IDF, and the implementation of multiple classification models. Each model is evaluated based on accuracy, classification reports, and confusion matrices to determine the most effective approach for email classification. Additionally, the program tests custom email samples to demonstrate the practical application of the trained models.\n",
    "\n",
    "Assumptions\n",
    "The provided email dataset (ham_spam.csv) is representative of typical spam and ham emails encountered in real-world scenarios.\n",
    "The features extracted using TF-IDF vectorization are sufficient to capture the necessary patterns and distinctions between spam and ham emails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61be63-9803-4526-a71c-cf6370bbac95",
   "metadata": {},
   "source": [
    "Read the dataset from the csv file into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "877e2b21-1152-4713-98b7-5f978b5204ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"ham_spam.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac78de9-f9e3-4bd3-bed5-f75dde52056f",
   "metadata": {},
   "source": [
    "Find dimensions of the dataframe (rows,columns) and display its fist few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7ed91e1a-97c8-4fd2-9251-47340cbd444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df.shape) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad529ace-733f-4098-8f33-7f8d12b589e9",
   "metadata": {},
   "source": [
    "Display value count for each category for column Category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "283dcb53-27c8-40de-bda4-6d0bf3ee375f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Category.value_counts()\n",
    "#df.groupby('Category').count() #alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42db73f6-3654-4070-9f90-d03b6e749a67",
   "metadata": {},
   "source": [
    "Separate out the Category column as labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d03a49e3-c087-4530-8012-eaa3aab95f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ham\n",
       "1     ham\n",
       "2    spam\n",
       "Name: Category, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['Category']\n",
    "y.head (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb625f9-8a2b-4098-ac03-d30fca76899b",
   "metadata": {},
   "source": [
    "Separate out the Message column as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eaf7e8b7-f0e4-49fa-afac-f316a5a32218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['Message']\n",
    "X.head (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a784f98-9133-451c-b428-24cc7f7160c6",
   "metadata": {},
   "source": [
    "Find the type of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7541bf3-0583-4f7b-910c-99e1fc5427ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1314bb-4968-469a-8587-20dff7d93b4d",
   "metadata": {},
   "source": [
    "Change the type of X from pandas series to list so as to be able to iterate on it using a for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c47d67b-2a2a-4316-bc92-bbda7029b511",
   "metadata": {},
   "source": [
    "Cleanup emails using regular epressions:\n",
    "- Remove all characters except alphabetical characters by replacing them with a space character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "55de7829-82eb-4e1e-8816-f5d3f9d4481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanup (text):   \n",
    "    text = re.sub ('[^a-zA-Z]', ' ', text)  # remove all characters that are not alphabets\n",
    "    text = re.sub (r'\\s+', ' ', text)  # remove extra white spaces and tabs\n",
    "    #r above indicates to python that its row string and not to interpret \n",
    "    #its escape characters instead pass it to the function as it is.\n",
    "    return text\n",
    "        \n",
    "X = X.apply(cleanup)  # apply the cleanup function to all the messages can also be rewritten as X = X.apply(lambda x: cleanup(x))\n",
    "\n",
    "# X = X.apply(cleanup) applies the cleanup function to each element (or each row/column) of X.\n",
    "# X = cleanup(X) applies the cleanup function to the entire object X in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "58acb68c-e117-41ca-82a5-75a1dde1ce94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Go until jurong point crazy Available only in ...\n",
       "1                                Ok lar Joking wif u oni \n",
       "2       Free entry in a wkly comp to win FA Cup final ...\n",
       "3            U dun say so early hor U c already then say \n",
       "4       Nah I don t think he goes to usf he lives arou...\n",
       "                              ...                        \n",
       "5567    This is the nd time we have tried contact u U ...\n",
       "5568                   Will b going to esplanade fr home \n",
       "5569    Pity was in mood for that So any other suggest...\n",
       "5570    The guy did some bitching but I acted like i d...\n",
       "5571                            Rofl Its true to its name\n",
       "Name: Message, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a5d0c-b43d-4434-83d8-f157143c0518",
   "metadata": {},
   "source": [
    "Vectorize content of all email using TfidfVectorizer vectorizer which will additionally do the following:\n",
    "- keep maximum 2000 words per email (max_features=2000) (discard the rest)\n",
    "- keep words which are present in at least in 5 emails (min_df=5) (discard irrelevant words)\n",
    "- keep words which are present in at most 70% of documents (max_df=0.7) (discard overly common words, retain relevant words) \n",
    "- remove all stop words (short words which don't affect meanings such as 'is', 'the' etc. These words are listed in nltk.corpus)\n",
    "\n",
    "At the end display one of the vectors and its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3f23a9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9a179d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/keithstateson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b1dc7451-dda3-433b-9d8b-f6e74056f11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(5572, 1624)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use the stopwords in the TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(max_features=2000, min_df=5,  max_df=0.7, \\\n",
    "                 stop_words=stopwords.words('english') )\n",
    "\n",
    "# Fit and transform the data\n",
    "X_vectorized = tfidf_vect.fit_transform(X).toarray()\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_vectorized[0:])\n",
    "\n",
    "# Print the shape of the transformed data\n",
    "# X_vect_list.shape  # error by professor\n",
    "print(X_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7adc8-a076-4ffa-ab3b-fa7477bc1777",
   "metadata": {},
   "source": [
    "Partition data into train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "51e658d7-7d44-4670-b491-21d045299664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split (X_vectorized, y,\\\n",
    "                                                     test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e65009-03d7-420c-9ff9-6b20c00fda36",
   "metadata": {},
   "source": [
    "Train the model using training data and its corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a82a8a3e-3d1c-45f8-a1ac-6401431f916b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_rf = RandomForestClassifier (n_estimators=500, random_state=0)\n",
    "clf_rf.fit(X_train,y_train)\n",
    "\n",
    "clf_knn = KNeighborsClassifier (n_neighbors=5)\n",
    "clf_knn.fit(X_train,y_train)\n",
    "\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6406e280-906d-4da7-ba56-07d2a7b93651",
   "metadata": {},
   "source": [
    "Test each model using test data and save its predicted results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a0e1e835-db6e-48a2-911a-32fce7aa542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "y_pred_knn = clf_knn.predict(X_test)\n",
    "y_pred_lr = clf_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0fa67-4ea9-42a2-857f-0633037e2d5e",
   "metadata": {},
   "source": [
    "Produce accuracy score, classification report, and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3ef76956-dbc8-417a-8c23-1aede41008f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9847533632286996\n",
      "KNN Accuracy: 0.9183856502242153\n",
      "Logistic Regression Accuracy: 0.9704035874439462\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       955\n",
      "        spam       0.99      0.90      0.94       160\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.95      0.97      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "\n",
      "KNN Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.91      1.00      0.95       955\n",
      "        spam       0.99      0.44      0.61       160\n",
      "\n",
      "    accuracy                           0.92      1115\n",
      "   macro avg       0.95      0.72      0.78      1115\n",
      "weighted avg       0.92      0.92      0.90      1115\n",
      "\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.98       955\n",
      "        spam       0.98      0.81      0.89       160\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.90      0.93      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n",
      "\n",
      "Random Forest Confusion Matrix:\n",
      " [[954   1]\n",
      " [ 16 144]]\n",
      "\n",
      "KNN Confusion Matrix:\n",
      " [[954   1]\n",
      " [ 90  70]]\n",
      "\n",
      "Logistic Regression Confusion Matrix:\n",
      " [[953   2]\n",
      " [ 31 129]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,\\\n",
    "confusion_matrix, accuracy_score\n",
    "\n",
    "print(f'Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf)}')\n",
    "print(f'KNN Accuracy: {accuracy_score(y_test, y_pred_knn)}')\n",
    "print(f'Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr)}')\n",
    "\n",
    "print(f'\\nRandom Forest Classification Report:\\n {classification_report(y_test, y_pred_rf)}')\n",
    "print(f'\\nKNN Classification Report:\\n {classification_report(y_test, y_pred_knn)}')\n",
    "print(f'\\nLogistic Regression Classification Report:\\n {classification_report(y_test, y_pred_lr)}')\n",
    "\n",
    "print(f'\\nRandom Forest Confusion Matrix:\\n {confusion_matrix(y_test, y_pred_rf)}')\n",
    "print(f'\\nKNN Confusion Matrix:\\n {confusion_matrix(y_test, y_pred_knn)}')\n",
    "print(f'\\nLogistic Regression Confusion Matrix:\\n {confusion_matrix(y_test, y_pred_lr)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4564c38-5b88-48f5-b562-3c9b412ac9f5",
   "metadata": {},
   "source": [
    "Below, Try a individual emails for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "99e4b47e-02ef-4a8c-8049-fc6db105b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = [\"Free entry in 2 a wkly comp to win final tickets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "51574456-ef4a-48cb-9a30-9f1d7a4b2eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# email_vectorized= tfidf_vect.transform(cleanup(email)).toarray()\n",
    "\n",
    "# If predicting one email\n",
    "# Ensure `cleanup` returns a string\n",
    "email_string = str(email)\n",
    "cleaned_email = cleanup(email_string)\n",
    "\n",
    "# Transform the cleaned email text. Wrap it in a list to make it an iterable.\n",
    "email_vectorized = tfidf_vect.transform([cleaned_email]).toarray()\n",
    "\n",
    "# Print the vectorized email\n",
    "print(email_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a38e6740-ea69-4920-bf86-6d8b9eb8c56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Prediction: ['spam']\n",
      "KNN Prediction: ['ham']\n",
      "Logistic Regression Prediction: ['ham']\n"
     ]
    }
   ],
   "source": [
    "y_pred_rf = clf_rf.predict(email_vectorized)\n",
    "y_pred_knn = clf_knn.predict(email_vectorized)\n",
    "y_pred_lr = clf_lr.predict(email_vectorized)\n",
    "\n",
    "print(f'Random Forest Prediction: {y_pred_rf}')\n",
    "print(f'KNN Prediction: {y_pred_knn}')\n",
    "print(f'Logistic Regression Prediction: {y_pred_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3a86a161-16e7-4457-b953-7dd01a1791f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "email2 = ['Congrats! 1 year special cinema pass for 2 Suprman V, Matrix3, StarWars3,\\\n",
    "etc all 4 FREE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a5267443-717e-4e6c-86b3-eb0bb9faf7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "email2_string = email2[0]\n",
    "\n",
    "cleaned_email2 = cleanup(email2_string)\n",
    "\n",
    "# Transform the cleaned email text. Wrap it in a list to make it an iterable.\n",
    "email2_vectorized = tfidf_vect.transform([cleaned_email2]).toarray()\n",
    "\n",
    "# Print the vectorized email\n",
    "print(email2_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5a329002-a128-4621-9be0-966b4d128f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Prediction: ['ham']\n",
      "KNN Prediction: ['spam']\n",
      "Logistic Regression Prediction: ['ham']\n"
     ]
    }
   ],
   "source": [
    "print(f'Random Forest Prediction: {clf_rf.predict(email2_vectorized)}')\n",
    "print(f'KNN Prediction: {clf_knn.predict(email2_vectorized)}')\n",
    "print(f'Logistic Regression Prediction: {clf_lr.predict(email2_vectorized)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e86e76fc-1492-43f4-a9ac-e3c1cb152086",
   "metadata": {},
   "outputs": [],
   "source": [
    "email3 = ['Time running out!! \\\n",
    "vacation to Hawaii. Stay in 4 star Hotel! Marriot, Hilton, etc. ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a851da5e-fac6-47f9-b30a-d02395289829",
   "metadata": {},
   "outputs": [],
   "source": [
    "email3_string = email3[0]\n",
    "cleaned_email3 = cleanup(email3_string)\n",
    "email3_vectorized = tfidf_vect.transform([cleaned_email3]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "edc95813-13d7-42af-846e-95ac5a1a793e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Prediction: ['ham']\n",
      "KNN Prediction: ['ham']\n",
      "Logistic Regression Prediction: ['ham']\n"
     ]
    }
   ],
   "source": [
    "print(f'Random Forest Prediction: {clf_rf.predict(email3_vectorized)}')\n",
    "print(f'KNN Prediction: {clf_knn.predict(email3_vectorized)}')\n",
    "print(f'Logistic Regression Prediction: {clf_lr.predict(email3_vectorized)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "03a82c82-70cd-4049-844a-05407c01416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = [[\"Free entry in 2 a wkly comp to win final tickets\"], ['Congrats! 1 year special cinema pass for 2 Suprman V, Matrix3, StarWars3,\\\n",
    "etc all 4 FREE!'], ['Time running out!! \\\n",
    "vacation to Hawaii. Stay in 4 star Hotel! Marriot, Hilton, etc. ']]\n",
    "cleaned_emails = [cleanup(email[0]) for email in emails]\n",
    "                         \n",
    "emails_vectorized= tfidf_vect.transform(cleaned_emails).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e6ac738d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Prediction: ['spam' 'ham' 'ham']\n",
      "KNN Prediction: ['ham' 'spam' 'ham']\n",
      "Logistic Regression Prediction: ['ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "print(f'Random Forest Prediction: {clf_rf.predict(emails_vectorized)}')\n",
    "print(f'KNN Prediction: {clf_knn.predict(emails_vectorized)}')\n",
    "print(f'Logistic Regression Prediction: {clf_lr.predict(emails_vectorized)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85690885",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "In this assignment, we developed and compared three different classification models for spam detection. The Random Forest model performed the best with the highest accuracy yet all three had excellent accuracy scores above 90%. Future improvements could include tuning hyperparameters and exploring more sophisticated NLP techniques such as word embeddings and deep learning models like RNN and CNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
