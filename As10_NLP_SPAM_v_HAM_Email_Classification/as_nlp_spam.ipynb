{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504084ed-1876-430c-ac89-0ff58c9a5ae4",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "## Text Classification \n",
    "\n",
    "## Predicting Text Category of Email (Ham or Spam)\n",
    "\n",
    "## Objectives\n",
    "\n",
    "On completing the assignment, you will be able to write a simple ai application to classify emails into spam or ham (not spam).\n",
    "\n",
    "## Description\n",
    " \n",
    "Write an AI application which will classify emails into spam and ham (not spam). For training and testing purposes, please use the dataset provided in the file, ham_spam.csv. The dataset contains 5572 sample emails labeled as ham or spam. Use 80% of the items for training, and the remaining 20% for testing. Use RandomForestClassifier classifier of sklearn.ensemble module with initial parameters, n_estimators=500, random_state=0 when creating the classifier object. After testing, produce accuracy score, classification report, and confusion matrix. Then, try out a few of your own created short emails and note the predicted response.\n",
    "\n",
    "### Additionally, do the following:\n",
    "\n",
    "#### Training model \n",
    "\n",
    "Train the following classifiication models of sklearn's library on the same data as described above and print their performance using the values of accuracy score.\n",
    "\n",
    "- RandomForestClassifier\n",
    "- KNeighborsClassifier\n",
    "- LogisticRegression\n",
    "\n",
    "#### Individual Values\n",
    "Also, try 3 made up values of emails with the best performing model and print the result.\n",
    "\n",
    "\n",
    "#### Code for using different classification model\n",
    "\n",
    "Below, X_train and y_train contain training data features and their corresponding labels respectively and X_test contains the testing data.\n",
    "\n",
    "RandomForestClassifier\n",
    "\n",
    "    #Train\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=500)\n",
    "    clfl.fit (X_train,y_train)\n",
    "\n",
    "    #Test\n",
    "    y_prdict = clf.predict(X_test)\n",
    "\n",
    "KNeighborsClassifier\n",
    "\n",
    "    #Train\n",
    "    from sklearn.neighbors  import KNeighborsClassifier\n",
    "    clf = KNeighborsClassifier(n_neighbors= 5)\n",
    "    clf.fit (X_train,y_train)\n",
    "\n",
    "    #Test\n",
    "    y_prdict = clf.predict(X_test)\n",
    "\n",
    "LogisticRegression\n",
    "\n",
    "    #Train\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression ()\n",
    "    clfl.fit (X_train,y_train)\n",
    "\n",
    "    # Test\n",
    "    y_prdict = clf.predict(X_test)\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Follow the step below\n",
    "\n",
    "### Data set (corpus) used for the application\n",
    "\n",
    "Use the data set (corpus) in the following file:\n",
    "\n",
    "\"ham_spam.csv\"\n",
    "\n",
    "### Load the data set\n",
    "\n",
    "Load the testing data as below\n",
    "\n",
    "```\n",
    "df=pd.read_csv(\"ham_spam.csv\")\n",
    "```\n",
    "\n",
    "### Assign Category column to y and Message column to y\n",
    "\n",
    "- Assign the Category column to variable y (the labels).\n",
    "- Assign the Message column to variable X (the features). \n",
    "\n",
    "### Clean up text in X\n",
    "\n",
    "- remove all special characters (remove all characters except alphabets, digits, and space characters). Short words such as 'it', 'the' etc. (stopwords) will be removed during vectorization. \n",
    "\n",
    "\n",
    "### Vectorize \n",
    "\n",
    "\n",
    "- Vectorize X by using TfidfVectorizer vectorizer of sklearn.feature_extraction.text module with parameters below and call it X_list_vectorized:\n",
    "  \n",
    "max_features=2000, min_df=5,  max_df=0.7, stop_words=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "### Split data\n",
    "\n",
    "Split data X_vectorized, y into X_train, X_test, y_train, and y_test data using train_test_split function of sklearn.model_selection module. Use 80% of the data for training and the rest for testing.\n",
    "\n",
    "### Train algorithm\n",
    "\n",
    "Train RandomForestClassifier classifier (initial parameter: n_estimators=500) of sklearn.ensemble module using the training data X_train and y_train\n",
    "\n",
    "### Test Algorithm\n",
    "\n",
    "Test the trained classifier with testing data X_test\n",
    "\n",
    "### Print Result\n",
    "\n",
    "Print accuracy score, classification report, and confusion matrix\n",
    "\n",
    "### Test Single Film Reviews\n",
    "\n",
    "Test a few made-up emails using the trained classifier and print the results.\n",
    "\n",
    "\n",
    "## Submittal\n",
    "\n",
    "The uploaded submittal should contain the following:\n",
    "\n",
    "- jpynb file after runninng the application from start to finish containing the marked source code, output, and your interaction.\n",
    "- the corresponding html file.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "In natural language problems, data is made up of sample documents (instead of sample values). Together these documents are called corpus (instead of data set). Since, our learning algorithms are based on numerical values, the documents in the corpus need to be encoded into numerical values (or numerical vectors) before using any algorithm.  \n",
    "\n",
    "#### Preprocessing\n",
    "\n",
    "Many words in the documents are not relevant to classifying the documents and can be excluded. In general, punctuation marks and other symbols are removed from the documents. Short words such as \"to\", \"on\", \"the\" etc. (called stop words) are also taken out. Furthermore, words with the same root such as eats, eating, ate, eaten, etc. are replaced with their common root words. So, documents go through a good deal of preprocessing before they are encoded into numerical values (numerical vectors). \n",
    "\n",
    "### Vectorizing Documents (Encoding documents into numerical values)\n",
    "\n",
    "The process of encoding documents into numerical values is called vectorizing because each document is encoded into a numerical vector (array of numerical values). The two common methods of vectorizing documents are \"Bag of Words\" (BOG) and \"Term Frequency Inverse Document Frequency\" (TFIDF) and they are described below.\n",
    "\n",
    "#### Bag of Words (BOW)\n",
    "\n",
    "In this method, each sample document is encoded into a numerical vector (numerical array) made up of several numerical values. \n",
    "\n",
    "##### Preparing vocabulary for the corpus\n",
    "\n",
    "In using this method, at first, we prepare vocabulary of all words used in the whole corpus (in all the sample documents in the data set) and assign each word a unique id (index) so that each word can be identified by its id (index). For example, if there are 200 different words used in the whole corpus, then its vocabulary is 200 words and each word is assigned a unique id (index) from 0 to 199.\n",
    "\n",
    "After that, we  assign to each document, a document vector (an array of numbers) of the same size as vocabulary size for the whole corpus. So, for a corpus with vocabulary size of 200 words, we assign each document, a 200 size numerical vector (numerical array) where the first value in the vector pertains to the word whose id (index) is 0, the second pertains to the word whose id (index) is 1, the third pertains to the word whose id (index) is 2, and so on. \n",
    "\n",
    "Then we start assigning values to vectors. In assigning values to a document vector, we start with the first value in the vector. This value pertains to the word whose id (index) is zero. So, in vocabulary, we lookup the word whose id (index) is zero. Then we go to the document and determine the frequency of use of that word (how many times this words has been used in the document). Then, we assign the frequency of use value as the value in the vector. (Note that if the word is never used in the document, its frequency of use is zero; if it is used once, its frequency of use is 1; if it is used twice, its frequency of use is 2;  and so on.) \n",
    "\n",
    "We repeat this process for every value in the vector and each time, we assign the frequency of use of the corresponding word in the document, as the value in the vector. Thus, each value in the document vector indicates the frequency of use of the corresponding word in the document. \n",
    "\n",
    "As an example of the above, see Example 1 below. In Example 1, our corpus is made up of three short sample documents, doc 1, doc 2, and doc 3, each containing a sentence. \n",
    "\n",
    "First we determine the vocabulary for the whole corpus. It comprises 8 words and is shown below. The ids (indices) of these words are also shown below and they are from 0 to 7.\n",
    "\n",
    "Then, we determine the vector for each document. \n",
    "\n",
    "For example, for doc 3, the first value of the vector pertains to the word whose id (index) is 0. From vocabulary, we find that the word with id (index) 0 is 'sue'. Then, we determine the frequency of use of the word 'sue' in doc 3. It is not used at all. Consequently, its frequency of use is zero. So, we assign 0 as the first value of the vector. \n",
    "\n",
    "Similarly, we determine the next value of the vector. The next value of the vector pertains to the word whose id (index) is 1. From vocabulary, we find that the word with id (index) 1 is 'is'. Then, we determine the frequency of use of the word 'is' in doc 3. The word 'is' is used in the document twice. Consequently, its frequency of use is 2. So, we assign 2 as the next value of the vector. \n",
    "\n",
    "We repeat this for determining other values of doc 3 vector. When all values are determined, the doc 3 vector values are: 0, 2, 2, 0, 1, 1, 1, 1 as shown below.\n",
    "\n",
    "##### Example 1\n",
    "\n",
    "              doc 1: sue is ok \n",
    "              doc 2: jim is ok \n",
    "              doc 3: sam is ok but joe is not ok \n",
    "\n",
    "              vocabulary:  sue is ok jim sam but joe not \n",
    "              word id's:    0   1  2  3   4   5   6   7 \n",
    "\n",
    "              doc 1 vector: 1   1  1  0   0   0   0   0 \n",
    "              doc 2 vector: 0   1  1  1   0   0   0   0 \n",
    "              doc 3 vector: 0   2  2  0   1   1   1   1 \n",
    "\n",
    "\n",
    "#### Term Frequency Inverse Document Frequency (TFIDF)\n",
    "\n",
    "This method of assigning a numerical vector to each document is identical to the method of 'Bag of Words' (BOW) described above except that, in the last step of assigning values to the vector, instead of assigning frequency of use values, we assign TFIDF values of the corresponding words.\n",
    "\n",
    "A TFIDF value of a word is calculated by multiplying its TF and IDF values as described below.\n",
    "\n",
    "##### Term Frequency (TF) value\n",
    "\n",
    "Term frequency of a word (TF) is equal to: (\"the frequency of the word in the document\" divided by \"the total number of words in the document\"). The concept behind TF is that the more frequent a word is in a document, the more it is relevant to the document. \n",
    "\n",
    "##### Inverse Document Frequency (IDF)\n",
    "\n",
    "Inverse Document Frequency (IDF) of a word is equal to: the log of (\"total number of documents in the corpus\" divided by \"the number of documents in which the word is used\"). When the corpus contains huge number of documents, the numerator and the quotient of the value in parentheses above can become very large. By taking a log of the value, the value of IDF is kept manageable. The concept behind IDF is that a word which is used in too many documents, such as the word 'the', that word is not relevant to the document. However, if a word is only used in a few documents, then it is relevant to those documents.  \n",
    "\n",
    "##### Combining TF and IDF\n",
    "\n",
    "TFIDF is obtained by multiplying TF and IDF. However, there is a variety of ways in which IDF and TFIDF are calculated and combined.\n",
    "\n",
    "#### An example of assigning TFIDF values \n",
    "\n",
    "For an example of assigning TFIDF values, see Example 2 below. Example 2 is identical to Example 1 above except that we are assigning TFIDF values to the document vectors in place of assigning the frequency of use values. \n",
    "\n",
    "The TFIDF vector values for the three documents doc 1, doc 2, and doc 3, are shown below. We describe below the process of determining values for doc 3 vector. \n",
    "\n",
    "The first value in the doc 3 vector pertains to the word whose id (index) is 0. From vocabulary, we find that the word with id (index) 0 is 'sue'. \n",
    "\n",
    "Since, word 'sue' is used 0 times (not used at all) in doc 3 out of a total of 8 words that make up 3, its TF value is 0 as shown below.\n",
    "\n",
    "TF (relative frequency of use) value for word 'sue' for doc 3 vector:\n",
    "frequency of use in doc 3 / total number of words in doc 3 = 0/8 = 0 \n",
    "\n",
    "TFIDF value for word 'sue': TF * IDF = 0 * IDF = 0\n",
    "\n",
    "Calculating in the same way, the first four values for doc 3 vector are 0.\n",
    "\n",
    "Now, we discuss the calculation for the fifth value of doc 3 vector. This value corresponds to the word whose id (index) is 4. Per vocabulary, that word is 'sam'. The calculation TFID for the word 'sam' for doc 3 vector are shown below.\n",
    "\n",
    "TF (relative frequency of use) value for word 'sam' in doc 3 vector:\n",
    "frequency of use in doc 3 / total number of words in doc 3 = 1/8 = 0.125 \n",
    "\n",
    "IDF (relative inverse document frequency) value for word 'sam' in doc 3 vector:\n",
    "log (total documents in the copus/number of documents containing 'sam')= \n",
    "log (3/1) = log 3 = 0.477 \n",
    "\n",
    "TFIDF value for word 'sam': TF * IDF = 0.125 * 0.477 = 0.06\n",
    "\n",
    "Similarly, the remaining values of doc 3 vector are .06 as indicated below.\n",
    "\n",
    "\n",
    "##### Example 2\n",
    "\n",
    "              doc 1: sue is ok \n",
    "              doc 2: jim is ok \n",
    "              doc 3: sam is ok but joe is not ok \n",
    "\n",
    "              vocabulary:   sue  is  ok  jim  sam  but  joe  not \n",
    "              word id's:    0    1   2   3    4    5    6    7 \n",
    "\n",
    "              doc 1 vector: .16  0   0   0    0    0    0    0 \n",
    "              doc 2 vector: 0    0   0  .16   0    0    0    0 \n",
    "              doc 3 vector: 0    0   0   0   .06  .06  .06  .06\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8385dd-3790-4a8c-a40a-5b34e2a969fa",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61be63-9803-4526-a71c-cf6370bbac95",
   "metadata": {},
   "source": [
    "Read the dataset from the csv file into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "877e2b21-1152-4713-98b7-5f978b5204ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"ham_spam.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac78de9-f9e3-4bd3-bed5-f75dde52056f",
   "metadata": {},
   "source": [
    "Find dimensions of the dataframe (rows,columns) and display its fist few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "7ed91e1a-97c8-4fd2-9251-47340cbd444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df.shape) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad529ace-733f-4098-8f33-7f8d12b589e9",
   "metadata": {},
   "source": [
    "Display value count for each category for column Category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "283dcb53-27c8-40de-bda4-6d0bf3ee375f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Category.value_counts()\n",
    "#df.groupby('Category').count() #alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42db73f6-3654-4070-9f90-d03b6e749a67",
   "metadata": {},
   "source": [
    "Separate out the Category column as labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "d03a49e3-c087-4530-8012-eaa3aab95f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ham\n",
       "1     ham\n",
       "2    spam\n",
       "Name: Category, dtype: object"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['Category']\n",
    "y.head (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb625f9-8a2b-4098-ac03-d30fca76899b",
   "metadata": {},
   "source": [
    "Separate out the Message column as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "eaf7e8b7-f0e4-49fa-afac-f316a5a32218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['Message']\n",
    "X.head (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a784f98-9133-451c-b428-24cc7f7160c6",
   "metadata": {},
   "source": [
    "Find the type of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "b7541bf3-0583-4f7b-910c-99e1fc5427ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1314bb-4968-469a-8587-20dff7d93b4d",
   "metadata": {},
   "source": [
    "Change the type of X from pandas series to list so as to be able to iterate on it using a for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c47d67b-2a2a-4316-bc92-bbda7029b511",
   "metadata": {},
   "source": [
    "Cleanup emails using regular epressions:\n",
    "- Remove all characters except alphabetical characters by replacing them with a space character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "55de7829-82eb-4e1e-8816-f5d3f9d4481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanup (text):   \n",
    "    text = re.sub ('[^a-zA-Z]', ' ', text) \n",
    "    text = re.sub (r'\\s+', ' ', text) \n",
    "    #r above indicates to python that its row string and not to interpret \n",
    "    #its escape characters instead pass it to the function as it is.\n",
    "    return text\n",
    "        \n",
    "X = X.apply(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "58acb68c-e117-41ca-82a5-75a1dde1ce94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Go until jurong point crazy Available only in ...\n",
       "1                                Ok lar Joking wif u oni \n",
       "2       Free entry in a wkly comp to win FA Cup final ...\n",
       "3            U dun say so early hor U c already then say \n",
       "4       Nah I don t think he goes to usf he lives arou...\n",
       "                              ...                        \n",
       "5567    This is the nd time we have tried contact u U ...\n",
       "5568                   Will b going to esplanade fr home \n",
       "5569    Pity was in mood for that So any other suggest...\n",
       "5570    The guy did some bitching but I acted like i d...\n",
       "5571                            Rofl Its true to its name\n",
       "Name: Message, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a5d0c-b43d-4434-83d8-f157143c0518",
   "metadata": {},
   "source": [
    "Vectorize content of all email using TfidfVectorizer vectorizer which will additionally do the following:\n",
    "- keep maximum 2000 words per email (max_features=2000) (discard the rest)\n",
    "- keep words which are present in at least in 5 emails (min_df=5) (discard irrelevant words)\n",
    "- keep words which are present in at most 70% of documents (max_df=0.7) (discard overly common words, retain relevant words) \n",
    "- remove all stop words (short words which don't affect meanings such as 'is', 'the' etc. These words are listed in nltk.corpus)\n",
    "\n",
    "At the end display one of the vectors and its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "b1dc7451-dda3-433b-9d8b-f6e74056f11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5572, 1624)"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(max_features=2000, min_df=5,  max_df=0.7, \\\n",
    "                 stop_words=stopwords.words('english') )\n",
    "X_vectorized= tfidf_vect.fit_transform(X).toarray()\n",
    "print (X_vectorized[0:])\n",
    "X_vect_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7adc8-a076-4ffa-ab3b-fa7477bc1777",
   "metadata": {},
   "source": [
    "Partition data into train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "51e658d7-7d44-4670-b491-21d045299664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split (X_vectorized, y,\\\n",
    "                                                     test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e65009-03d7-420c-9ff9-6b20c00fda36",
   "metadata": {},
   "source": [
    "Train the model using training data and its corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "a82a8a3e-3d1c-45f8-a1ac-6401431f916b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=500, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=500, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, random_state=0)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf=RandomForestClassifier (n_estimators=500, random_state=0)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6406e280-906d-4da7-ba56-07d2a7b93651",
   "metadata": {},
   "source": [
    "Test the model using test data and save its predicted results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "a0e1e835-db6e-48a2-911a-32fce7aa542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0fa67-4ea9-42a2-857f-0633037e2d5e",
   "metadata": {},
   "source": [
    "Produce accuracy score, classification report, and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "3ef76956-dbc8-417a-8c23-1aede41008f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9847533632286996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       955\n",
      "        spam       0.99      0.90      0.94       160\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.95      0.97      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "[[954   1]\n",
      " [ 16 144]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,\\\n",
    "confusion_matrix, accuracy_score\n",
    "\n",
    "print(accuracy_score (y_test, y_pred))\n",
    "print (classification_report (y_test, y_pred))\n",
    "print (confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4564c38-5b88-48f5-b562-3c9b412ac9f5",
   "metadata": {},
   "source": [
    "Below, Try a individual emails for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "99e4b47e-02ef-4a8c-8049-fc6db105b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = [\"Free entry in 2 a wkly comp to win final tickets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "51574456-ef4a-48cb-9a30-9f1d7a4b2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_vectorized= tfidf_vect.transform(do_cleanup(email)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "a38e6740-ea69-4920-bf86-6d8b9eb8c56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], dtype=object)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(email_vectorized)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "3a86a161-16e7-4457-b953-7dd01a1791f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "email2 = ['Congrats! 1 year special cinema pass for 2 Suprman V, Matrix3, StarWars3,\\\n",
    "etc all 4 FREE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "a5267443-717e-4e6c-86b3-eb0bb9faf7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "email2_vectorized= tfidf_vect.transform(do_cleanup(email2)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "5a329002-a128-4621-9be0-966b4d128f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype=object)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(email2_vectorized)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "e86e76fc-1492-43f4-a9ac-e3c1cb152086",
   "metadata": {},
   "outputs": [],
   "source": [
    "email3 = ['Time running out!! \\\n",
    "vacation to Hawaii. Stay in 4 star Hotel! Marriot, Hilton, etc. ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "a851da5e-fac6-47f9-b30a-d02395289829",
   "metadata": {},
   "outputs": [],
   "source": [
    "email3_vectorized = tfidf_vect.transform(do_cleanup(email3)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "edc95813-13d7-42af-846e-95ac5a1a793e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype=object)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(email3_vectorized)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a82c82-70cd-4049-844a-05407c01416b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
