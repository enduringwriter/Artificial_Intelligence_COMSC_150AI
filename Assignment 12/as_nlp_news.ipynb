{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "960c7976-a2eb-496f-ba42-b6a748ac272f",
   "metadata": {},
   "source": [
    "## Text Classification \n",
    "\n",
    "### Predicting Category\n",
    "\n",
    "### Objectives\n",
    "\n",
    "On completing the assignment, you will learn how to write a simple text classification ai application.\n",
    "\n",
    "### Description\n",
    " \n",
    "Write an AI application which, given a sentence, will predict as to which category the text of the sentence belongs: sports, religion, autos, space, electronics etc. For training and testing purposes, please use the labeled dataset provided in the file, newsgroups.csv. The dataset contains 5572 news items from 20 different news categories. The news items are labeled with categories numbers 0 to 19. However, another file, news_cat_names.csv, is provided that contains the corresponding names of the categories. So, you can obtain a category name by indexing the category number into category name list. Use 80% of the data items for training, and the remaining 20% for testing. After testing, produce accuracy score, classification report, and confusion matrix. Then, try out a few of your own and note the category predicted by the application.\n",
    "\n",
    "### Implementatation\n",
    "\n",
    "### Data set (corpus) used for the application\n",
    "\n",
    "Use the data set (corpus) in the following files:\n",
    "\n",
    "\"newsgroups.csv\" and news_cat_names.csv\n",
    "\n",
    "The first file above contains 5572 news items from 20 different news categories. The news categories are labeled 0 to 19.\n",
    "\n",
    "The second file contains the names of news categories, The category names in the second file match in order of category number 0 to 19 in the first file. So, you can access the category name of a particular category by using the category number as an index.\n",
    "\n",
    "### Load the data set\n",
    "\n",
    "Loaded the data files into pandas data frames as shown below.\n",
    "\n",
    "    df=pd.read_csv(\"newsgroups.csv\", index_col=0)\n",
    "    df_cat=pd.read_csv(\"news_categories.csv\", index_col=0)\n",
    "\n",
    "### Assign Category column to y and Message column to X\n",
    "\n",
    "- Assign the Category column to variable y (the labels).\n",
    "- Assign the Message column to variable X (the features). \n",
    "\n",
    "### Vectorize data\n",
    "\n",
    "Vectorize data using TfidfVectorizer. The data passed to the vectorizer should be either a pandas Series object or list object or similar iterable object. If it is plain text, just covert it to a list object. The code for the vectorizer is given below where X is a Series object (or a list of text) that need to be vectorized.\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer(max_features=2000, min_df=5,  max_df=0.7, \\\n",
    "                 stop_words=stopwords.words('english') )\n",
    "    X_vectorized= tfidf_vect.fit_transform(X).toarray()\n",
    "\n",
    "### Split data\n",
    "\n",
    "Split data X_list_vectorized, y into X_train, X_test, y_train, and y_test data using train_test_split function of sklearn.model_selection module. Use 80% of the data for training and the rest for testing.\n",
    "\n",
    "### Train algorithm\n",
    "\n",
    "Train MultinomialNB classifier (of sklearn.naive_bayes module using the training data X_train and y_train as below:\n",
    "\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    clf = MultinomialNB()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "\n",
    "### Test Algorithm\n",
    "\n",
    "Test the trained classifier with testing data X_test as below\n",
    "\n",
    "    y_predict = clf.predict(X_test)\n",
    "\n",
    "### Print Result\n",
    "\n",
    "Print accuracy score, classification report, and confusion matrix\n",
    "\n",
    "### Test your own sentences\n",
    "\n",
    "Test at least 6 made-up sentences using the vectorizer and trained classifier and print the resultant category as a number and as a name.\n",
    "\n",
    "For vectorizing, you would need to convert your quoted text into a list by enclosing it within the square brackets [ ] before passing it to vectorizer.\n",
    "\n",
    "A few suggested sentences are below:\n",
    "\n",
    "    'He is a catholic',\n",
    "    'Who will be the next US president',\n",
    "    'We should drive our cars safely',\n",
    "    'Television needs electicity to work'\n",
    "\n",
    "Below is the example code\n",
    "\n",
    "    input= \"He is a catholic\"\n",
    "    df_cat.Category_Names[clf.predict(vect.transform([input]))]\n",
    "\n",
    "The above gave the following output:\n",
    "15    Christianity\n",
    "Name: Category_Names, dtype: object\n",
    "\n",
    "When done, put all of your selected sentences as a list in a variable as shown below. Then vecterize and test them as a group and print the result as a group both as category numbers and names\n",
    "\n",
    "    input_data = ['He is a catholic',\n",
    "    'Who will be the next US president',\n",
    "    'We should drive our cars safely',\n",
    "    'Television needs electicity to work']\n",
    "\n",
    "### Submittal\n",
    "\n",
    "The uploaded submittal should contain the following:\n",
    "\n",
    "### Coding\n",
    "\n",
    "Follow steps similar to those in spam email application.\n",
    "\n",
    "### Submittal\n",
    "\n",
    "The uploaded submittal should contain the following:\n",
    "\n",
    "- jpynb file after runninng the application from start to finish containing the marked source code, output, and your interaction.\n",
    "- the corresponding html file.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "The discussion on classifying text documents follows.\n",
    "\n",
    "####  Introduction\n",
    "\n",
    "Many words in the documents are not relevant to classifying the documents and can be excluded. In general, punctuation marks and other symbols are removed from the documents. Short words such as \"to\", \"on\", \"the\" etc. (called stop words) are also taken out. Furthermore, words with the same root such as eats, eating, ate, eaten, etc. are replaced with their common root words. So, documents go through a good deal of preprocessing before they are encoded into numerical values (numerical vectors). \n",
    "\n",
    "#### Vectorizing Documents (Encoding documents into numerical values)\n",
    "\n",
    "The process of encoding documents into numerical values is called vectorizing because each document is encoded into a numerical vector (array of numerical values). The two common methods of vectorizing documents are \"Bag of Words\" (BOG) and \"Term Frequency Inverse Document Frequency\" (TFIDF) and they are described below.\n",
    "\n",
    "#### Bag of Words (BOW)\n",
    "\n",
    "In this method, each sample document is encoded into a numerical vector (numerical array) made up of several numerical values. \n",
    "\n",
    "##### Preparing vocabulary for the corpus\n",
    "\n",
    "In using this method, at first, we prepare vocabulary of all words used in the whole corpus (in all the sample documents in the data set) and assign each word a unique id (index) so that each word can be identified by its id (index). For example, if there are 200 different words used in the whole corpus, then its vocabulary is 200 words and each word is assigned a unique id (index) from 0 to 199.\n",
    "\n",
    "After that, we  assign to each document, a document vector (an array of numbers) of the same size as vocabulary size for the whole corpus. So, for a corpus with vocabulary size of 200 words, we assign each document, a 200 size numerical vector (numerical array) where the first value in the vector pertains to the word whose id (index) is 0, the second pertains to the word whose id (index) is 1, the third pertains to the word whose id (index) is 2, and so on. \n",
    "\n",
    "Then we start assigning values to vectors. In assigning values to a document vector, we start with the first value in the vector. This value pertains to the word whose id (index) is zero. So, in vocabulary, we lookup the word whose id (index) is zero. Then we go to the document and determine the frequency of use of that word (how many times this words has been used in the document). Then, we assign the frequency of use value as the value in the vector. (Note that if the word is never used in the document, its frequency of use is zero; if it is used once, its frequency of use is 1; if it is used twice, its frequency of use is 2;  and so on.) \n",
    "\n",
    "We repeat this process for every value in the vector and each time, we assign the frequency of use of the corresponding word in the document, as the value in the vector. Thus, each value in the document vector indicates the frequency of use of the corresponding word in the document. \n",
    "\n",
    "As an example of the above, see Example 1 below. In Example 1, our corpus is made up of three short sample documents, doc 1, doc 2, and doc 3, each containing a sentence. \n",
    "\n",
    "First we determine the vocabulary for the whole corpus. It comprises 8 words and is shown below. The ids (indices) of these words are also shown below and they are from 0 to 7.\n",
    "\n",
    "Then, we determine the vector for each document. \n",
    "\n",
    "For example, for doc 3, the first value of the vector pertains to the word whose id (index) is 0. From vocabulary, we find that the word with id (index) 0 is 'sue'. Then, we determine the frequency of use of the word 'sue' in doc 3. It is not used at all. Consequently, its frequency of use is zero. So, we assign 0 as the first value of the vector. \n",
    "\n",
    "Similarly, we determine the next value of the vector. The next value of the vector pertains to the word whose id (index) is 1. From vocabulary, we find that the word with id (index) 1 is 'is'. Then, we determine the frequency of use of the word 'is' in doc 3. The word 'is' is used in the document twice. Consequently, its frequency of use is 2. So, we assign 2 as the next value of the vector. \n",
    "\n",
    "We repeat this for determining other values of doc 3 vector. When all values are determined, the doc 3 vector values are: 0, 2, 2, 0, 1, 1, 1, 1 as shown below.\n",
    "\n",
    "##### Example 1\n",
    "\n",
    "              doc 1: sue is ok \n",
    "              doc 2: jim is ok \n",
    "              doc 3: sam is ok but joe is not ok \n",
    "\n",
    "              vocabulary:  sue is ok jim sam but joe not \n",
    "              word id's:    0   1  2  3   4   5   6   7 \n",
    "\n",
    "              doc 1 vector: 1   1  1  0   0   0   0   0 \n",
    "              doc 2 vector: 0   1  1  1   0   0   0   0 \n",
    "              doc 3 vector: 0   2  2  0   1   1   1   1 \n",
    "\n",
    "\n",
    "#### Term Frequency Inverse Document Frequency (TFIDF)\n",
    "\n",
    "This method of assigning a numerical vector to each document is identical to the method of 'Bag of Words' (BOW) described above except that, in the last step of assigning values to the vector, instead of assigning frequency of use values, we assign TFIDF values of the corresponding words.\n",
    "\n",
    "A TFIDF value of a word is calculated by multiplying its TF and IDF values as described below.\n",
    "\n",
    "##### Term Frequency (TF) value\n",
    "\n",
    "Term frequency of a word (TF) is equal to: (\"the frequency of the word in the document\" divided by \"the total number of words in the document\"). The concept behind TF is that the more frequent a word is in a document, the more it is relevant to the document. \n",
    "\n",
    "##### Inverse Document Frequency\n",
    "\n",
    "Inverse Document Frequency (IDF) of a word is equal to: the log of (\"total number of documents in the corpus\" divided by \"the number of documents in which the word is used\"). When the corpus contains huge number of documents, the numerator and the quotient of the value in parentheses above can become very large. By taking a log of the value, the value of IDF is kept manageable. The concept behind IDF is that a word which is used in too many documents, such as the word 'the', that word is not relevant to the document. However, if a word is only used in a few documents, then it is relevant to those documents.  \n",
    "\n",
    "##### Combining TF and IDF\n",
    "\n",
    "TFIDF is obtained by multiplying TF and IDF. However, there is a variety of ways in which IDF and TFIDF are calculated and combined.\n",
    "\n",
    "#### An example of assigning TFIDF values \n",
    "\n",
    "For an example of assigning TFIDF values, see Example 2 below. Example 2 is identical to Example 1 above except that we are assigning TFIDF values to the document vectors in place of assigning the frequency of use values. \n",
    "\n",
    "The TFIDF vector values for the three documents doc 1, doc 2, and doc 3, are shown below. We describe below the process of determining values for doc 3 vector. \n",
    "\n",
    "The first value in the doc 3 vector pertains to the word whose id (index) is 0. From vocabulary, we find that the word with id (index) 0 is 'sue'. \n",
    "\n",
    "Since, word 'sue' is used 0 times (not used at all) in doc 3 out of a total of 8 words that make up 3, its TF value is 0 as shown below.\n",
    "\n",
    "TF (relative frequency of use) value for word 'sue' for doc 3 vector:\n",
    "frequency of use in doc 3 / total number of words in doc 3 = 0/8 = 0 \n",
    "\n",
    "TFIDF value for word 'sue': TF * IDF = 0 * IDF = 0\n",
    "\n",
    "Calculating in the same way, the first four values for doc 3 vector are 0.\n",
    "\n",
    "Now, we discuss the calculation for the fifth value of doc 3 vector. This value corresponds to the word whose id (index) is 4. Per vocabulary, that word is 'sam'. The calculation TFID for the word 'sam' for doc 3 vector are shown below.\n",
    "\n",
    "TF (relative frequency of use) value for word 'sam' in doc 3 vector:\n",
    "frequency of use in doc 3 / total number of words in doc 3 = 1/8 = 0.125 \n",
    "\n",
    "IDF (relative inverse document frequency) value for word 'sam' in doc 3 vector:\n",
    "log (total documents in the copus/number of documents containing 'sam')= \n",
    "log (3/1) = log 3 = 0.477 \n",
    "\n",
    "TFIDF value for word 'sam': TF * IDF = 0.125 * 0.477 = 0.06\n",
    "\n",
    "Similarly, the remaining values of doc 3 vector are .06 as indicated below.\n",
    "\n",
    "\n",
    "##### Example 2\n",
    "\n",
    "              doc 1: sue is ok \n",
    "              doc 2: jim is ok \n",
    "              doc 3: sam is ok but joe is not ok \n",
    "\n",
    "              vocabulary:   sue  is  ok  jim  sam  but  joe  not \n",
    "              word id's:    0    1   2   3    4    5    6    7 \n",
    "\n",
    "              doc 1 vector: .16  0   0   0    0    0    0    0 \n",
    "              doc 2 vector: 0    0   0  .16   0    0    0    0 \n",
    "              doc 3 vector: 0    0   0   0   .06  .06  .06  .06\n",
    "\n",
    "\n",
    "- jpynb file after runninng the application from start to finish containing the marked source code, output, and your interaction.\n",
    "- the corresponding html file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaedede-dc4b-4855-8dc9-7e654111a59a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
