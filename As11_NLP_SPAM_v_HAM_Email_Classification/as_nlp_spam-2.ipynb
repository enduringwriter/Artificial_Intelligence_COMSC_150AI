{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504084ed-1876-430c-ac89-0ff58c9a5ae4",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "## Text Classification \n",
    "\n",
    "### Predicting Text Category of Email (Ham or Spam)\n",
    "\n",
    "### Objectives\n",
    "\n",
    "On completing the assignment, you will able to write a simple ai application to classify emails into spam or ham (not spam).\n",
    "\n",
    "### Discription\n",
    "\n",
    "This assignment is similar to the last assignment except with the following provissions.\n",
    "\n",
    "1. For vectorizing, use the Bag of Words (BOW) model. The vectorizer  code is shown below. \n",
    "\n",
    "        from nltk.corpus import stopwords\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "        vect = CountVectorizer(max_features=2000, min_df=5,  max_df=0.7, \\\n",
    "                 stop_words=stopwords.words('english') )\n",
    "\n",
    "        X_vectorized = vect.fit_transform(X).toarray()\n",
    "\n",
    "2. For training and testing, try the following classification models. The code for the models is also shown below.\n",
    "\n",
    "- Multimonial Naïve Bayes classifier\n",
    "- RandomForestClassifier\n",
    "- SVC - Support Vector Classifier\n",
    "\n",
    "Code for the above is shown below. X_trail and y_train contains the training code and X_test contains the testing code. \n",
    "\n",
    "Multimonial Naïve Bayes classifier\n",
    "\n",
    "    #Train\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    #Test\n",
    "    y_prdict = clf.predict(X_test)\n",
    "\n",
    "RandomForestClassifier\n",
    "\n",
    "    #Train\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=500)\n",
    "    clfl.fit (X_train,y_train)\n",
    "\n",
    "    #Test\n",
    "    y_prdict = clf.predict(X_test)\n",
    "\n",
    "SVC - Support Vector Classifier\n",
    "\n",
    "    #Train\n",
    "    from sklearn.svm import SVC\n",
    "    clf = SVC()\n",
    "    clfl.fit (X_train,y_train)\n",
    "    \n",
    "    #Test\n",
    "    y_prdict = clf.predict(X_test)\n",
    "\n",
    "3. For the best performing model, in addition to accuracy_score, print out its classification report, and confusion matrix.\n",
    "\n",
    "4. Individual Values\n",
    "\n",
    "Also, try out made-up emails with the best performing model from the above list and print the emails and the results. \n",
    "\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Discussion on classifying text document follows.\n",
    "\n",
    "#### Preprocessing\n",
    "\n",
    "Many words in the documents are not relevant to classifying the documents and can be excluded. In general, punctuation marks and other symbols are removed from the documents. Short words such as \"to\", \"on\", \"the\" etc. (called stop words) are also taken out. Furthermore, words with the same root such as eats, eating, ate, eaten, etc. are replaced with their common root words. So, documents go through a good deal of preprocessing before they are encoded into numerical values (numerical vectors). \n",
    "\n",
    "#### Vectorizing Documents (Encoding documents into numerical values)\n",
    "\n",
    "The process of encoding documents into numerical values is called vectorizing because each document is encoded into a numerical vector (array of numerical values). The two common methods of vectorizing documents are \"Bag of Words\" (BOG) and \"Term Frequency Inverse Document Frequency\" (TFIDF) and they are described below.\n",
    "\n",
    "#### Bag of Words (BOW)\n",
    "\n",
    "In this method, each sample document is encoded into a numerical vector (numerical array) made up of several numerical values. \n",
    "\n",
    "##### Preparing vocabulary for the corpus\n",
    "\n",
    "In using this method, at first, we prepare vocabulary of all words used in the whole corpus (in all the sample documents in the data set) and assign each word a unique id (index) so that each word can be identified by its id (index). For example, if there are 200 different words used in the whole corpus, then its vocabulary is 200 words and each word is assigned a unique id (index) from 0 to 199.\n",
    "\n",
    "After that, we  assign to each document, a document vector (an array of numbers) of the same size as vocabulary size for the whole corpus. So, for a corpus with vocabulary size of 200 words, we assign each document, a 200 size numerical vector (numerical array) where the first value in the vector pertains to the word whose id (index) is 0, the second pertains to the word whose id (index) is 1, the third pertains to the word whose id (index) is 2, and so on. \n",
    "\n",
    "Then we start assigning values to vectors. In assigning values to a document vector, we start with the first value in the vector. This value pertains to the word whose id (index) is zero. So, in vocabulary, we lookup the word whose id (index) is zero. Then we go to the document and determine the frequency of use of that word (how many times this words has been used in the document). Then, we assign the frequency of use value as the value in the vector. (Note that if the word is never used in the document, its frequency of use is zero; if it is used once, its frequency of use is 1; if it is used twice, its frequency of use is 2;  and so on.) \n",
    "\n",
    "We repeat this process for every value in the vector and each time, we assign the frequency of use of the corresponding word in the document, as the value in the vector. Thus, each value in the document vector indicates the frequency of use of the corresponding word in the document. \n",
    "\n",
    "As an example of the above, see Example 1 below. In Example 1, our corpus is made up of three short sample documents, doc 1, doc 2, and doc 3, each containing a sentence. \n",
    "\n",
    "First we determine the vocabulary for the whole corpus. It comprises 8 words and is shown below. The ids (indices) of these words are also shown below and they are from 0 to 7.\n",
    "\n",
    "Then, we determine the vector for each document. \n",
    "\n",
    "For example, for doc 3, the first value of the vector pertains to the word whose id (index) is 0. From vocabulary, we find that the word with id (index) 0 is 'sue'. Then, we determine the frequency of use of the word 'sue' in doc 3. It is not used at all. Consequently, its frequency of use is zero. So, we assign 0 as the first value of the vector. \n",
    "\n",
    "Similarly, we determine the next value of the vector. The next value of the vector pertains to the word whose id (index) is 1. From vocabulary, we find that the word with id (index) 1 is 'is'. Then, we determine the frequency of use of the word 'is' in doc 3. The word 'is' is used in the document twice. Consequently, its frequency of use is 2. So, we assign 2 as the next value of the vector. \n",
    "\n",
    "We repeat this for determining other values of doc 3 vector. When all values are determined, the doc 3 vector values are: 0, 2, 2, 0, 1, 1, 1, 1 as shown below.\n",
    "\n",
    "##### Example 1\n",
    "\n",
    "              doc 1: sue is ok \n",
    "              doc 2: jim is ok \n",
    "              doc 3: sam is ok but joe is not ok \n",
    "\n",
    "              vocabulary:  sue is ok jim sam but joe not \n",
    "              word id's:    0   1  2  3   4   5   6   7 \n",
    "\n",
    "              doc 1 vector: 1   1  1  0   0   0   0   0 \n",
    "              doc 2 vector: 0   1  1  1   0   0   0   0 \n",
    "              doc 3 vector: 0   2  2  0   1   1   1   1 \n",
    "\n",
    "\n",
    "#### Term Frequency Inverse Document Frequency (TFIDF)\n",
    "\n",
    "This method of assigning a numerical vector to each document is identical to the method of 'Bag of Words' (BOW) described above except that, in the last step of assigning values to the vector, instead of assigning frequency of use values, we assign TFIDF values of the corresponding words.\n",
    "\n",
    "A TFIDF value of a word is calculated by multiplying its TF and IDF values as described below.\n",
    "\n",
    "##### Term Frequency (TF) value\n",
    "\n",
    "Term frequency of a word (TF) is equal to: (\"the frequency of the word in the document\" divided by \"the total number of words in the document\"). The concept behind TF is that the more frequent a word is in a document, the more it is relevant to the document. \n",
    "\n",
    "##### Inverse Document Frequency (IDF)\n",
    "\n",
    "Inverse Document Frequency (IDF) of a word is equal to: the log of (\"total number of documents in the corpus\" divided by \"the number of documents in which the word is used\"). When the corpus contains huge number of documents, the numerator and the quotient of the value in parentheses above can become very large. By taking a log of the value, the value of IDF is kept manageable. The concept behind IDF is that a word which is used in too many documents, such as the word 'the', that word is not relevant to the document. However, if a word is only used in a few documents, then it is relevant to those documents.  \n",
    "\n",
    "##### Combining TF and IDF\n",
    "\n",
    "TFIDF is obtained by multiplying TF and IDF. However, there is a variety of ways in which IDF and TFIDF are calculated and combined.\n",
    "\n",
    "#### An example of assigning TFIDF values \n",
    "\n",
    "For an example of assigning TFIDF values, see Example 2 below. Example 2 is identical to Example 1 above except that we are assigning TFIDF values to the document vectors in place of assigning the frequency of use values. \n",
    "\n",
    "The TFIDF vector values for the three documents doc 1, doc 2, and doc 3, are shown below. We describe below the process of determining values for doc 3 vector. \n",
    "\n",
    "The first value in the doc 3 vector pertains to the word whose id (index) is 0. From vocabulary, we find that the word with id (index) 0 is 'sue'. \n",
    "\n",
    "Since, word 'sue' is used 0 times (not used at all) in doc 3 out of a total of 8 words that make up 3, its TF value is 0 as shown below.\n",
    "\n",
    "TF (relative frequency of use) value for word 'sue' for doc 3 vector:\n",
    "frequency of use in doc 3 / total number of words in doc 3 = 0/8 = 0 \n",
    "\n",
    "TFIDF value for word 'sue': TF * IDF = 0 * IDF = 0\n",
    "\n",
    "Calculating in the same way, the first four values for doc 3 vector are 0.\n",
    "\n",
    "Now, we discuss the calculation for the fifth value of doc 3 vector. This value corresponds to the word whose id (index) is 4. Per vocabulary, that word is 'sam'. The calculation TFID for the word 'sam' for doc 3 vector are shown below.\n",
    "\n",
    "TF (relative frequency of use) value for word 'sam' in doc 3 vector:\n",
    "frequency of use in doc 3 / total number of words in doc 3 = 1/8 = 0.125 \n",
    "\n",
    "IDF (relative inverse document frequency) value for word 'sam' in doc 3 vector:\n",
    "log (total documents in the copus/number of documents containing 'sam')= \n",
    "log (3/1) = log 3 = 0.477 \n",
    "\n",
    "TFIDF value for word 'sam': TF * IDF = 0.125 * 0.477 = 0.06\n",
    "\n",
    "Similarly, the remaining values of doc 3 vector are .06 as indicated below.\n",
    "\n",
    "\n",
    "##### Example 2\n",
    "\n",
    "              doc 1: sue is ok \n",
    "              doc 2: jim is ok \n",
    "              doc 3: sam is ok but joe is not ok \n",
    "\n",
    "              vocabulary:   sue  is  ok  jim  sam  but  joe  not \n",
    "              word id's:    0    1   2   3    4    5    6    7 \n",
    "\n",
    "              doc 1 vector: .16  0   0   0    0    0    0    0 \n",
    "              doc 2 vector: 0    0   0  .16   0    0    0    0 \n",
    "              doc 3 vector: 0    0   0   0   .06  .06  .06  .06\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f09461-ab8c-4cf8-927e-be62197f2b14",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61be63-9803-4526-a71c-cf6370bbac95",
   "metadata": {},
   "source": [
    "Read the dataset from the csv file into a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac78de9-f9e3-4bd3-bed5-f75dde52056f",
   "metadata": {},
   "source": [
    "Find dimensions of the dataframe (rows,columns) and display its fist few lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad529ace-733f-4098-8f33-7f8d12b589e9",
   "metadata": {},
   "source": [
    "Display value count for each category for column Category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a784f98-9133-451c-b428-24cc7f7160c6",
   "metadata": {},
   "source": [
    "Find the type of X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1314bb-4968-469a-8587-20dff7d93b4d",
   "metadata": {},
   "source": [
    "Change the type of X from pandas series to list so as to be able to iterate on it using a for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c47d67b-2a2a-4316-bc92-bbda7029b511",
   "metadata": {},
   "source": [
    "Cleanup emails (X-list) using regular epressions:\n",
    "- Remove all characters except alphabetical characters by replacing them with a space character.\n",
    "- Remove all one character words by replacing them with space.\n",
    "- Replace multiple spaces with single space\n",
    "\n",
    "At the end, display a few emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a5d0c-b43d-4434-83d8-f157143c0518",
   "metadata": {},
   "source": [
    "Vectorize content of all email using TfidfVectorizer vectorizer which will additionally do the following:\n",
    "- keep maximum 2000 words per email (max_features=2000) (discard the rest)\n",
    "- keep words which are present in at least in 5 emails (min_df=5) (discard irrelevant words)\n",
    "- keep words which are present in at most 70% of documents (max_df=0.7) (discard overly common words, retain relevant words) \n",
    "- remove all stop words (short words which don't affect meanings such as 'is', 'the' etc. These words are listed in nltk.corpus)\n",
    "\n",
    "At the end display one of the vectors and its length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e65009-03d7-420c-9ff9-6b20c00fda36",
   "metadata": {},
   "source": [
    "Train the model using training data and its corresponding labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6406e280-906d-4da7-ba56-07d2a7b93651",
   "metadata": {},
   "source": [
    "Test the model using test data and save its predicted results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0fa67-4ea9-42a2-857f-0633037e2d5e",
   "metadata": {},
   "source": [
    "Produce accuracy score, classification report, and confusion matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
