{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931793e0-95db-4202-aaec-8222707f084c",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "## Text Summarization \n",
    "\n",
    "## Objectives\n",
    "\n",
    "On completing this assignment, students will be able to write a simple ai application that summarizes a given text by selecting a few most relevant sentences from the text.  \n",
    "\n",
    "## Description\n",
    " \n",
    "Write an AI application that will scrape a Wikipedia article on Neural Networking from the Internet and will summarize it by selecting the three most relevant sentences which are less than 20 words long from the article.\n",
    "\n",
    "### Additionally, do the following:\n",
    "\n",
    "Allow sentences of the following maximum length to be included in the calculations and see which one produces a good summary.\n",
    "\n",
    "Max sentence length:\n",
    "15, 20, 25, 30, or any length\n",
    "\n",
    "After selecting a suitable length for the above, try out the following numbers for sentences to be included in the final summary.\n",
    "\n",
    "Number of sentences to be included in the document summary:\n",
    "1, 2, 3, 4, or 5\n",
    "\n",
    "What number seems most suitable. \n",
    "\n",
    "Write a paragraph that both describes your experience and summarizes the results of carrying out the above experiment.\n",
    "\n",
    "\n",
    "## Discussion\n",
    "\n",
    "There are two ways to summarize an article. One way is to fully comprehend the article and then summarize it in your own words. This way we produce an abstract of the article. The second way is to extract from the article a few most relevant sentences and use them to constitute the summary. This type of summary is called an executive summary.\n",
    "\n",
    "In this assignment, we have chosen the second approach of producing an executive summary of the article. \n",
    "\n",
    "## Coding\n",
    "\n",
    "Follow the steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "5f532508-e771-4667-a758-65de43a7e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "raw_data = urllib.request.urlopen ('https://en.wikipedia.org/wiki/Neural_network')\n",
    "#print (raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b20af-9388-4f66-94d5-7ac502edf243",
   "metadata": {},
   "source": [
    "Read the raw page from the connected website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "b73544ab-ddf1-4249-93d3-d4e4fee51f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=raw_data.read()\n",
    "#print (document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab242e-3dc9-40ad-bb62-1004daf98fc3",
   "metadata": {},
   "source": [
    "Cleanup the page to make it a clean html page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "7f35549e-db98-43e7-84af-5f1cdcb6fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_document = bs.BeautifulSoup(document, 'lxml')\n",
    "#print (parsed_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050cbdd-e331-4f2c-a8b8-ddcb3c9cc1cb",
   "metadata": {},
   "source": [
    "Prepare a list of all <p> tag objects (<p> tags and the enclosed text) (html paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "76a89a8d-a8ee-4259-a7d3-3d17f08b4f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_paras=parsed_document.find_all ('p')\n",
    "#print (article_paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6173a5af-4aff-4537-94b4-ea5fc9ee0aa7",
   "metadata": {},
   "source": [
    "By iterating over the list, extract and put together the text parts (html paragraph text par) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "3c538ca0-af99-4243-9a9e-b6256330c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapped_data=\"\"\n",
    "for para in article_paras:\n",
    "    scrapped_data += para.text\n",
    "#print (scrapped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434613e-e37a-4ce4-9bf1-6e036696e7a4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "At the end of text parts, there are reference numbers such as: [1] etc. Do the cleanup of the whole text and remove them as done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "a7d09a3e-2db0-4205-b30e-8a38311c231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "scrapped_data = re.sub (r'\\[[0-9]*\\]', ' ', scrapped_data)\n",
    "scrapped_data = re.sub (r'\\s+', ' ', scrapped_data)\n",
    "#print(scrapped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94836caa-3113-4b0f-a3e4-277274d8059b",
   "metadata": {},
   "source": [
    "Tokenize (surround with single quotes) all sentences and make a list of them in which quoted sentences are separated by comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "190d7c26-5999-4945-8b0d-8c9e018d1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "all_sentences = sent_tokenize (scrapped_data)\n",
    "#print (all_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bff50a-ca1b-47e9-bd4b-c313cc8b78de",
   "metadata": {},
   "source": [
    "Start with the data in which the data is not yet sentence tokenized and prepare a word frequency list (dictionary) for all the data. [Word frequency list is a list (dictionary) that contains words and their corresponding frequencies (the number of times the words are used in the document)]. Do this the the following way:\n",
    "\n",
    "- cleanup the document so that it contains only alphabetic text\n",
    "- tokenize words (surround each word with single quotes and put them in a list)\n",
    "- iterate on the tokenized word list and prepare a word frequency list (dictionary) while making sure not to include stopwords (short words such as 'to', 'is', etc.) in the frequency count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "6e2d5451-9386-475d-aa4f-b614e661077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "scrapped_data = re.sub ('[^a-zA-Z]', ' ', scrapped_data)\n",
    "formatted_text = re.sub (r'\\s+', ' ', scrapped_data)\n",
    "#print (formatted_text)\n",
    "\n",
    "word_freq = {}\n",
    "for word in word_tokenize (formatted_text):\n",
    "    if word not in stopwords.words('english'):\n",
    "        if word not in word_freq.keys():\n",
    "            word_freq [word] = 1\n",
    "        else:\n",
    "            word_freq [word] += 1\n",
    "\n",
    "#print (word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af516ee2-144e-4c1d-97f9-8b1ae4a8b2bf",
   "metadata": {},
   "source": [
    "Convert the word frequencies to relative word frequencies by dividing them all to maximum frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "da5e0867-2491-4cc2-b203-251650a4e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq=max(word_freq.values())\n",
    "for word in word_freq.keys():\n",
    "    word_freq [word]= word_freq [word] / max_freq\n",
    "#print (word_freq)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de189dbe-55b4-4f97-9af1-e4f3bd2c31b5",
   "metadata": {},
   "source": [
    "After finished preparing the sentence list and relative word frequency list (dictionary) above, now prepare a sentence score list (dictionary) containing each sentence and its corresponding sentence score. (The sentence score is calculated by adding the word frequencies (from the word frequency list/dictionary) of all the words that make up the sentence.) Exclude the sentences that are 20 or more words long. Do this in the following way:\n",
    "\n",
    "iterate on the sentence list:   \n",
    "    if the sentence is less than 20 word long:\n",
    "        for each sentence, iterate on its freshly prepared tokenized wordlist:\n",
    "            if the word is in the word frequency list (dictionary):\n",
    "                if the sentence score exist in the sentence score list (dictionary):\n",
    "                    add the word frequency to the sentence score \n",
    "                else:\n",
    "                    initialize sentence score with the word frequency value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "10019859-ebce-486f-a65a-a9d8ed8529f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_scores = {}\n",
    "for sent in all_sentences:\n",
    "    if len (sent.split(' ')) < 25:\n",
    "        for word in word_tokenize (sent):\n",
    "            if word in word_freq.keys():            \n",
    "                if sent in sent_scores.keys():\n",
    "                    sent_scores [sent] += word_freq [word]\n",
    "                else:\n",
    "                    sent_scores [sent] = word_freq [word]\n",
    "\n",
    "#print (sent_scores)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a06f690-55b8-43d8-863b-50520591388f",
   "metadata": {},
   "source": [
    "From the sentence score list (dictionary), extract three sentences with top three scores and put them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "c1ca4a04-7245-42b8-ad10-a634049cb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "selected_sentences = heapq.nlargest(3, sent_scores, sent_scores.get)\n",
    "#print (selected_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd56c6d-f0be-47dd-b6eb-b69af586017d",
   "metadata": {},
   "source": [
    "Convert the above list of sentences into printable quoted text. Then print the quoted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "b78e210a-bb6b-4b8b-a3fb-e1b401bb31f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. Populations of interconnected neurons that are smaller than neural networks are called neural circuits. A neural network is a group of interconnected units called neurons that send signals to one another.\n"
     ]
    }
   ],
   "source": [
    "selected_summary = \" \".join (selected_sentences)\n",
    "print (selected_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38fdcc6-3c50-4a0f-a613-47cf8769abf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
